<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
   <title>ephjos micro</title>
   <description>micro</description>
   <language>en-us</language>
   <link>http://ephjos.io/micro/feed.xml</link>
   <atom:link href="http://ephjos.io/micro/feed.xml" rel="self" type="application/rss+xml" />


      <item>
        <title>ray tracing in a weekend</title>
        <link>http://ephjos.io/micro/2022/04/03</link>
        <guid>http://ephjos.io/micro/2022/04/03</guid>
        <pubDate>Sun, 03 Apr 2022 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>           Welp, everything outlined in           <a href="/micro/2022/01/28">habits</a> definitely did not happen.           Something something best laid plans of mice and men.         </p>          <p>           I have been distracted with a few things, mostly playing around with           WebGL. I was pulled down the           <a href="https://www.shadertoy.com/">ShaderToy</a> rabbit hole and           have been playing around with all kinds of ideas. Most notably, I           implemented the           <a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">             Ray Tracing in One Weekend</a> project in a fragment shader, which           you will see below. I learned quite a bit about WebGL, passing data           to shaders, and pseudo-random number generation in shaders. The below           example uses a fixed canvas size of 256x256, stretched with CSS. There           are 73 balls placed randomly in the scene each time that the shader           is compiled, which happens every page load. All of the objects are           written as constant structs into the shader, and the main loop           is unrolled. The compiled shader is           logged to the console. Each ray is allowed to bounce 50 times. Also,           this appears to be a path tracer (to my understanding), since we are           taking random rays when we bounce, instead of the exact reflection.           I implemented some basic WebGL abstractions           and the progressive rendering using some of the logic in           <a href="https://github.com/evanw/webgl-path-tracing">             evanw's webgl-path-tracing</a>.         </p>          <div class="container">           <canvas id="glcanvas" width="256" height="256"></canvas>           <div id="overlay">             <div id="size"></div>             <div id="fps"></div>             <div id="frame"></div>           </div>           <div></div>         </div>          <p>           I also found           <a href="https://github.com/sschoenholz/WebGL-Raytracer">             sschoenholz's WebGL-Raytracer</a> which does some cool texture           packing to pass the scene information to the shader.         </p>          <p>           All-in-all, I learned quite a bit about WebGL, raytracing, and path           tracing which makes me interested in doing more. I am not interested in           trying to add anymore complex features to my existing python           implementation of the Ray Tracer Challenge, so I think it is           time for a rewrite in a lower-level language.         </p>          <hr />          While I have been better with using my phone less (thanks to locking         myself out from the internet using         <a href="https://freedom.to/">Freedom</a>), I still need to         get back in the groove with reading and writing.  
        ]]></description>
      </item>
    

      <item>
        <title>monty hall</title>
        <link>http://ephjos.io/micro/2022/03/15</link>
        <guid>http://ephjos.io/micro/2022/03/15</guid>
        <pubDate>Tue, 15 Mar 2022 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I watched a great video from 3Blue1Brown today, and it reminded me         of what it was like to first wrap my head around the Monty Hall problem         as a CS undergrad. The video,         <a href="https://www.youtube.com/watch?v=OkmNXy7er84">The Hardest Problem on the Hardest Test</a>,         is about a question from a Putnam Competition. Finding the solution is         a matter of reframing the problem and changing perspective. Once viewed         from the correct lens, the answer seems trivial.         </p>          <p>           From           <a href="/micro/2021/10/26">my post on 10/26/2021</a> when           discussing the           <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">             Monty Hall problem</a>.            <blockquote>             Instead of focusing on the probability that             you pick the prize, focus on the probability that you initially pick a             goat. If 2/3 of the time you pick a goat, and you are always shown one             goat, switching results in you picking the car 2/3 of the time.           </blockquote>         </p>          <p>           I know I've discussed this before on this blog, but this skill is           something that I have been working to develop ever since exposed to           it. In CS, we spend a ton of work on formulating efficient data           structures and algorithms and then reframe the problem to fit           these abstractions. This gets to the core of what I find so           interesting about computer science, and seeing another example           of it in the wild is always intriguing.         </p>  
        ]]></description>
      </item>
    

      <item>
        <title>pi day</title>
        <link>http://ephjos.io/micro/2022/03/14</link>
        <guid>http://ephjos.io/micro/2022/03/14</guid>
        <pubDate>Mon, 14 Mar 2022 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>           In celebration of Pi Day, I figured that I could get around to           implementing a simulation of           <a href="https://en.wikipedia.org/wiki/Buffon%27s_needle_problem">             Buffon's Needle</a> for approximating pi. I've implemented           this in client-side JS and used WebGL for rendering. It's been           a while since I used WebGL directly and wrote some vanilla JS.           The           <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API/Tutorial/Using_shaders_to_apply_color_in_WebGL">             Mozilla docs for WebGL</a> have tons of great examples that           can be easily used as a jumping off point. This was a           fun exercise, and it was interesting to work with this first hand.         </p>         <p>           The implementation is far from optimal and does all of the heavy           lifting in JS. This could clearly be made more efficient, which           will be left as an exercise for the reader. Play with the parameters           and see how long it takes to generate a basic approximation of pi!         </p>         <div class="container">           <canvas id="glcanvas"></canvas>           <div id="overlay"></div>           <div>             <label>               Lines:               <input id="lines-slider" type="range" min="2" max="20" value="5" step="1">             </label>             <label>               Sticks (10^x):               <input id="sticks-slider" type="range" min="0" max="6" value="3" step="1">             </label>             <label>               Stick length:               <input id="stick-length-slider" type="range" min="0.1" max="1" value="0.4" step="0.1">             </label>             <label>               <button id="generate">Generate</button>             </label>             <label>               Find match to 4 decimals (3.1415):               <button id="find">Start</button>             </label>           </div>         </div>         <p>           If you want to check out a better version of this simulation with           more details on the math, check out           <a href="https://ogden.eu/pi/">Thomas Ogden's website</a>.         </p>  
        ]]></description>
      </item>
    

      <item>
        <title>the crucible</title>
        <link>http://ephjos.io/micro/2022/01/31</link>
        <guid>http://ephjos.io/micro/2022/01/31</guid>
        <pubDate>Mon, 31 Jan 2022 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>           During a visit to Salem Massachusetts last year, I stumbled onto           Arthur Miller's The Crucible in a bookshop. I figured           there wouldn't be a better place to grab a copy, so I bought it and           forgot about it.         </p>         <p>           All in all, I enjoyed the story. It was quick read and I           enjoyed that most of the characters were ripped straight from           actual events that occurred during the witch trials. Knowing about           Giles Corey and his pressing makes the impact of it in the story           much more intense. Remembering the spot where it occurred, how the           townspeople would have gathered to watch him be slowly crushed.           Recognizing a character's name from his real gravestone was an           impactful experience, and one I hope I don't repeat.         </p>         <p>           The frantic           dialog and the general absurdity made the play chaotic and           engaging, and definitely made its message clear. As an allegory for           McCarthyism, Miller did a great job of showing the lengths           people will go to when they believe they are morally correct.           The pervasiveness of this behavior pattern across time and           place is alarming, and fits in to many situations we face in           modern life.         </p>         <p>           To those in the midst of this mania, everything seems clear: the           Devil has fallen upon Salem. Only           with time does the chaos and confusion of the past become clear.         </p>  
        ]]></description>
      </item>
    

      <item>
        <title>habits</title>
        <link>http://ephjos.io/micro/2022/01/28</link>
        <guid>http://ephjos.io/micro/2022/01/28</guid>
        <pubDate>Fri, 28 Jan 2022 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>           Another year, another holiday season that I allowed to completely           wreck my schedule and good habits. Right around mid-December, I           lost all discipline. I started snacking and eating more sweets,           sleeping in more, playing video games, stopped doing Advent of Code           and other side projects, stopped writing, stopped reading, and           started spending more time on my phone. I need all of this           to change, so here are my immediate goals for the next month.         </p>          <ul>           <li>Finish Advent of Code (and related post)</li>           <li>Finish the Ray Tracer Challenge in Python</li>           <li>Start the Ray Tracer Challenge in C</li>           <li>Finish reading current book</li>           <li>Start and finish next book</li>           <li>One blog post</li>           <li>Consistent micro posts (1-2 per week)</li>         </ul>          <p>In order to reach these, I need to make some behavioral changes.</p>          <ul>           <li>Read every day before bed</li>           <li>Work on projects every day</li>           <li>Write every day after working on projects</li>           <li>Work out every morning</li>           <li>Leave phone in a designated spot when not actively using it</li>         </ul>  
        ]]></description>
      </item>
    

      <item>
        <title>leverage based todo list</title>
        <link>http://ephjos.io/micro/2021/12/04</link>
        <guid>http://ephjos.io/micro/2021/12/04</guid>
        <pubDate>Sat, 04 Dec 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         Well its been a month since I started using this todo list every day         (as mentioned in <a href="/micro/2021/11/04">this post</a>), and I         am still sticking with it.          <p>         I found this idea while reading         <a href="http://www.effectiveengineer.com/">           The Effective Engineer by Edmond Lau.</a> This was a great book         overall, and think that it provides a lot of valuable information         about software development in an easy to digest format. It has a         definite Silicon Valley slant, which is acknowledged by the author, but         may put off some readers/make some of the advice not as applicable in         certain situations. For me, the biggest takeaway is the core idea         of using leverage for decision making.          <p>         Leverage for any given task is the         <i><b>value produced per unit time</b></i>.         This provides a concrete way to analyze what tasks should be focused on,         given that one defines their "value" well. A key dimension to consider         is time, be it delayed or repeated returns. It is obvious we should         work on important and urgent tasks (Customer A needs feature X by the         end of the week). However, just as (if not more) vital are         important but non-urgent tasks. These are critical activities         that do not find their way into the main plan, but can spell         disaster if unaddressed. For most teams, these are problems like         scalability, code quality, and hiring. All of these do not provide         much immediate value, but shine when emphasized ahead of time. They         can prevent failures and enable new growth and activities for entire         teams and companies.         Ignoring         them can lead to an entire project grinding to a halt to do a lack of         talent, unmaintable code, or the inability to support a growing user         base.          <p>         Leverage can be applied at the individual level when using todo lists.         Instead of keeping         a sorted list of tasks, have 1 main task and a backlog of tasks.         Periodically perform a pairwise comparison between the main task         and every task in the backlog. If there is a task that provides higher         leverage and covers the switching costs, then consider switching to it.         Otherwise, work on the main task until is complete, then select the         next highest leverage task and move on.          <p>         I have tried todo lists before, but often found myself unable to         stick with it as I'd spend more time curating my list than working on         tasks. This method provides me a low mental overhead while also         giving me enough structure to complete my tasks. I've actually stuck to         this method, and plan to stick with it for the foreseeable future.         All personal and work tasks get thrown on the list, and get crossed         off when their done.          <p>         Software-wise, I've tried to keep it stupid simple and stay away from         more robust (agile-style) applications. I've done todo lists on paper         before, but found myself frustrated when I had an idea or remembered         something but didn't have my notebook handy. I've used apps that distract         with their complexity or didn't provide the same experience on desktop         and mobile. The tool I've been sticking with this time is         <a href="https://teuxdeux.com">TeuxDeux.</a>          <p>         Intentionally restricted         and minimalist, this app provides all of the functionality I need         and nothing more. This flows perfectly with the leverage based todo list         described above, and allow me to separate but keep track of current         tasks and backlog tasks. "Someday lists" can be used for ideas, tasks         that will need to be done in the future, or short lived lists like         grocery and shopping lists. The only other feature I need is the         ability to schedule recurring tasks, which is also built in. There is         not much else to distract me from focusing on crossing off tasks,         which is my main goal here.          <p>         Overall, I think the approach to todo lists outlined by Edmond Lau         is exceptional, especially when combined with the right tool.         I'd recommend anyone who works in or around software engineering         to check out the Effective Engine, and hope you find it as useful         as I have.   
        ]]></description>
      </item>
    

      <item>
        <title>juvenoia</title>
        <link>http://ephjos.io/micro/2021/11/28</link>
        <guid>http://ephjos.io/micro/2021/11/28</guid>
        <pubDate>Sun, 28 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         I read an excellent post today from Morgan Housel called         <a href="https://www.collaborativefund.com/blog/experts/">           Experts From A World That No Longer Exists</a>. The core idea         here is that the work required to become an expert in a field         also builds up resistance to certain ideas. Experts are often         against trying ideas that have failed in the past and are quick         to caution new-comers about these ideas. Inversely, new-comers         do not have the established knowledge and experience which may         limit their overall ability but leaves their thoughts completely         unobstructed. The idea was driven home for me with the examples         given from the dot-com boom; business ideas that were shunned         and failed have since become multi-billion dollar businesses         like Chewy and Amazon. One must allow their skills to develop         to an expert level while still seeing the world as a newbie, or         watch the world that they are an expert in cease to exist.         Those of us who refuse to try something since it has already         been tried before will miss out on opportunities that arise         as our world changes around us.          <p>         One idea that is closely tied in with this article is that         of         <a href="https://en.wiktionary.org/wiki/juvenoia">           Juvenoia</a>, a word used to describe an older generation's         irrational fear of the younger one. In this context, this         pertains to the expert's fear and misunderstanding of the         new-comers. Just because you can't understand them and/or         they are different does not inherently mean that their         actions or ideas are worthless. If anything, some of the         most radical changes of the past century have come from a new         generation taking something that "belonged" to their         forefathers and making it their own. Rock and roll, hip hop,         and first person shooters are all things born out of putting         a spin on something that existed before. At the time,         the old generation recoiled at these changes, saying that the         "kids these days are doomed".          <p>         Don't get stuck screaming about your lawn; develop the skills of an         expert but maintain a childish view of the world. Only a fool         would think anything is possible.  
        ]]></description>
      </item>
    

      <item>
        <title>pypy is cool</title>
        <link>http://ephjos.io/micro/2021/11/25</link>
        <guid>http://ephjos.io/micro/2021/11/25</guid>
        <pubDate>Thu, 25 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         In the last post I talked about the performance of the ray tracer         I am writing in python and found myself quite unhappy. I did some         digging into alternate interpreters and found         <a href="https://www.pypy.org/">           pypy</a> which does JIT compilation and focuses on long running         computationally intense tasks. It sounded up for the job, so I installed         it, got the right pip version and installed some packages, then ran it.          <p>         The cover image from the last post took ~17 minutes to render at 300px         by 300px. With pypy, the same image is rendered in ~4 minutes. In about         ~20 minutes, I was able to render it in 800px by 800px. This is all         a substantial improvement, and I am happy that it'll help speed up         my work until I get to more optimizations. I am also able to push         the rewrite in a compiled language into the future as I don't         think it'll be required for me to finish the book and render some         images.          <hr />          <p>         Today is Thanksgiving, and I am deeply thankful for all of the people         and things that make life worth living. I feel grateful for life         itself and everything that comes with it, the good and the bad. Being         alive is a great thing.          <p>         Happy Thanksgiving!  
        ]]></description>
      </item>
    

      <item>
        <title>pretty pictures, slowly</title>
        <link>http://ephjos.io/micro/2021/11/23</link>
        <guid>http://ephjos.io/micro/2021/11/23</guid>
        <pubDate>Tue, 23 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         I've hit a milestone with my implementation of the Ray Tracer         Challenge, I can render the cover image of the book!         <img           alt="cover image"           src="/img/rtc/cover.webp"           width="300"           height="300">          <p>         This was my primary goal when I first started this book, so I am pretty         stoked to have made it here. However, I am having major performance         issues. The above image took 17 minutes to render, which is not         great. It is significantly slowed down by using a ray bounce depth of         5, so every ray that hits a reflective surface has the potential         to 5x the computation, which adds up. I've applied various         optimizations already like caching inverse matrices, reducing         allocations by mutation, and rewriting the core tuple and matrix         code to be more efficient. These all provided a significant speed up,         but it is not quite enough. I've experimented with multithreading,         and saw a ~2x speed up, but I want to keep this single threaded.           <p>         Here's what the profiler output while generating the above image         (using snakeviz for visualization).          <img           alt="python profile graph screenshot"           src="/img/rtc/prof1.webp"           width="1385"           height="778">          <p>         As we can see, there is nothing unexpected. We spend the vast         majority of the time multiplying matrices together. I've focused         on this code, and can't squeeze much more performance out of it.         I experimented a bit with numpy and numba here, but didn't see much         improvement (and don't want to use them in this project). The problem         seems to be that we are wasting a lot of work. I've read ahead about         using a Bounded Volume Hierarchy to only check for intersections         on objects within a bounding box, and I look forward to the         potential speedup. As it is now, the above image required         172,535,400 calls to the <code>matrix_mul_tuple</code> function to be rendered in         300px by 300px. The book notes about an order of magnitude reduction in         the amount of intersection calls when using BVH on an appropriate scene,         which I hope to see as well.          <p>         I plan to implement BVH then crunch through the rest of the book         pretty quickly, given that I've done this all before. I'll explore         further optimizations at the end, and then look into doing another         implementation with performance in mind. I've been particularly         motivated by a site I found on the book's forum that         <a href="https://iliathoughts.com/posts/raytracer/">           demos a C++ implementation compiled to WASM</a>. If I can get         on the same order of magnitude of performance, I'll be happy.          <p>         I've been enjoying the process lately, which is great! While this         ray tracer is not the most performant (as admitted by the author), it         does produce         some of the better images that I have seen when compared to other         hobbyist/educational ray tracers. I am happy with the image quality         now, but I want to focus on performance before doing more post-book         additions like depth-of-field and anti-aliasing. For now, here's         two scenes from the recent chapters:          <img           alt="reflections and refractions image"           src="/img/rtc/reflect-refract.webp"           width="800"           height="800">         <img           alt="table cube example image"           src="/img/rtc/table.webp"           width="800"           height="800">  
        ]]></description>
      </item>
    

      <item>
        <title>lunar eclipse</title>
        <link>http://ephjos.io/micro/2021/11/19</link>
        <guid>http://ephjos.io/micro/2021/11/19</guid>
        <pubDate>Fri, 19 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                 <p>         As I was standing in my backyard this morning at 4am, watching the         lunar eclipse, I realized that its been a long time since I stared         into the night sky. Even in a light-polluted residential neighborhood,         the amount of stars you can see once your eyes adjust is incredible.         Its been a while since I've felt that weird feeling of cosmic scale.         I felt awe when it hit me that I was standing in the same shadow         as the moon. It all looks so far away yet so close.          <p>         The most incredible thing I noticed is that stars twinkle.          <p>         <i>I forgot that stars twinkle</i>          <p>         I've been so disconnected from the night sky that this just wasn't in         my head. I found the little dipper and started looking around         for other constellations when this hit me over the head. The         immensity coupled with understanding that the Earth         is the most special place in the known universe always make         me feel insignificant         but unbelievably lucky; lucky to be alive on this         <a href="https://en.wikipedia.org/wiki/Pale_Blue_Dot">           Pale Blue Dot</a>.          <p>         I need to stargaze more.   
        ]]></description>
      </item>
    

      <item>
        <title>the wrong abstraction</title>
        <link>http://ephjos.io/micro/2021/11/18</link>
        <guid>http://ephjos.io/micro/2021/11/18</guid>
        <pubDate>Thu, 18 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         Sandi Metz has a great short article called         <a href="https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction">           The Wrong Abstraction</a>. As software developers, we spend a lot         of our time and effort on abstractions. All of us understand what         it is like to run up against a bad abstraction that just doesn't         seem to fit the problem. The interesting angle that Sandi takes         is to <i>prefer duplication over the wrong abstraction</i>, due         to the disproportionate cost of bad abstractions. This is an         interesting idea that makes for an approach that only adds         abstractions once they are needed and obvious, or when the code         becomes <i>too</i> repetitive. To untangle an existing abstraction,         simply locate the places the abstraction is used and copy/paste         the abstraction inline. This provides the benefit of immediately         removing the dependency while providing the ability to more readily         identify and define a better abstraction.          <p>         All in all, this post is based on a simple idea, but it is a slight         shift in the way to approach problems that most of us are taught.         Instead of focusing on the existing abstraction and fitting the         problem to it, focus on removing bad/unneeded abstractions and only         add them where they truly provide value.  
        ]]></description>
      </item>
    

      <item>
        <title>parkinson's law</title>
        <link>http://ephjos.io/micro/2021/11/16</link>
        <guid>http://ephjos.io/micro/2021/11/16</guid>
        <pubDate>Tue, 16 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <blockquote>           work expands so as to fill the time available for its completion         </blockquote>          <p>         I stumbled onto the Wikipedia page for         <a href="https://en.wikipedia.org/wiki/Parkinson%27s_law">           Parkinson's law</a> today, and was intrigued. I had never heard this         name used for describing this specific phenomenon.         This aligns with the idea of scope creep which often comes up in         software engineering. We often view extra resources as an opportunity         to do more, which can be extremely detrimental to the goal of the         project.          <p>         This concept was referenced regarding this great post about         <a href="https://mzrn.sh/2021/11/14/how-i-helped-build-a-profitable-mvp-over-a-weekend"> building a profitable MVP in a weekend</a>.         This MVP was successful due to the owner being well connected and         experienced in the space, but also due to an aggressively small         scope. Instead of focusing on what is possible in a weekend and         listing out features, the author instead focused on whittling         down the app to the smallest possible size. This narrow focus allowed         them to ship in a weekend and start making money.          <p>         This is something that I am trying to apply to my projects, both         before and during them. I have become much more comfortable with         erasing ideas from my project list before starting them. Once started,         I actively work to <i>reduce</i> the amount of work remaining. This         leads to me focusing on more important tasks while working on and         finishing them more consistently.          <p>         The space of projects I <i>could</i>         do is infinite, but my time is not. Being able to pick out the most         valuable and fun projects from that space will give me the highest         yield for my effort. Once started, instead of drumming up new features         to add, it is almost always a better idea to start something new.  
        ]]></description>
      </item>
    

      <item>
        <title>docker development container for node</title>
        <link>http://ephjos.io/micro/2021/11/12</link>
        <guid>http://ephjos.io/micro/2021/11/12</guid>
        <pubDate>Fri, 12 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         Using a docker container for node.js development is a good idea for         a few reasons. It easily allows the development environment to be         shared with others and provides <i>some</i> amount of separation         from the host system. While far from being locked-down, running         <code>npm install</code> inside of the container can help prevent         install scripts from exploiting the host machine. Given how common         <a href="https://github.com/advisories?query=severity%3Acritical">           these types of vulnerabilities</a> have become, this is something         that any npm user should be aware of.          <p>         The setup is simple, consisting of a <code>Dockerfile</code>,         <code>docker-compose.yml</code>, and a single <code>package.json</code>         script. This will let you spin up the container and get a shell inside         of it to install packages and run commands. It can be further modified         to automatically run a command and be used for deployment, but I'll         leave that as an exercise for the reader.          <h3>Dockerfile</h3>         <pre><code>FROM node:latest WORKDIR /app/ COPY package.json . RUN npm install COPY . .</pre></code>          <h3>docker-compose.yml</h3>         <pre><code>version: '3.8' services:   project:     build:       context: .     command: bash     volumes:       - .:/app/       - /app/node_modules</pre></code>          <h3>package.json</h3>         <p>         You can use an existing package.json, or create one for yourself by         running <code>npm init</code>.         <pre><code>...   "scripts": {     ...     "docker:shell": "docker-compose run project",     ...   }, ...</pre></code>          <p>         Once that's setup, run <code>docker-compose up --build -d</code> to         start the container and then <code>npm run docker:shell</code>         to get a shell inside of it. From their <code>npm install</code>         and everything is up and running! Get out of the container by running         <code>exit</code> inside of it. Use         <code>docker-compose down -v</code> outside of the container         to completely tear it down if you want a clean start.          <p>         If you plan to run a server, you need to be sure to expose and use         the proper ports. Thanks to the container being mounted in the project         directory, you can use your favorite editor to create and modify files         on your host system while running them on the container.          <p>         This is far from perfect, but is a step in the right direction         and should provide some value to the developer and <i>maybe</i>         some small amount of security.  
        ]]></description>
      </item>
    

      <item>
        <title>distractions</title>
        <link>http://ephjos.io/micro/2021/11/11</link>
        <guid>http://ephjos.io/micro/2021/11/11</guid>
        <pubDate>Thu, 11 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         Lately, I've been going through a weird distraction cycle with this         website. Every few days, I'll convince myself that I have to         make some massive change to the project's structure, introduce         new technologies, and further complicate things. Down the rabbit         hole I go for a couple hours before clarity bonks me over the head.         "I don't need this" I think to myself as I realize the time I just         wasted.          <p>         As stated before, I get distracted by new tools and features incredibly         easily. By intentionally keeping the scope of this site small and         boring overall, I give myself more time to actually fill it with         content. I am hoping that I can shorten these distraction cycles         further, hopefully to the point where they stop happening. Here's some         reminders for myself.          <ul>           <li>This site does not need markdown</li>           <li>The most you have ever written is with this current setup</li>           <li>The deploy process does not need new tools brought into it</li>           <li>The goal here is to write and stay minimal, not to tinker with new toys</li>           <li>If I want to try something out, I can do it separate from this site</li>         </ul>        <p>Here are some changes that I will consider for the future:        <ul>         <li>           More efficient builds (don't rebuild index pages and RSS feeds from           scratch every time)         </li>         <li>           Scope index pages to N most recent posts, create separate archive           page with all of the posts to keep indexes small         </li>         <li>           Develop a simple tool to do SSI at build time, reduce the work of           the server.         </li>       </ul>        <p>       In reality, these are all fairly low leverage activities but they all       provide some value towards efficiency and keeping things clean. For now,       none of that is needed and I'd rather spend my effort on writing and       other projects.  
        ]]></description>
      </item>
    

      <item>
        <title>reorganizing into python modules</title>
        <link>http://ephjos.io/micro/2021/11/09</link>
        <guid>http://ephjos.io/micro/2021/11/09</guid>
        <pubDate>Tue, 09 Nov 2021 12:00:00 -0500</pubDate>
        <description><![CDATA[
                <p>         I take back what I said the other day about reorganizing my         python code for the Ray Tracer Challenge; I moved the code into         modules. Its been a while since I've done this, so there were plenty         of things I got wrong, but I am happy with it now. I broke the         code into three directories: rtc, test, demo. This allowed me to         break tests and demos out into separate files, which prevents         the circular import issues. The tests can be run using         the unittest module's discover functionality. The demos         are run by running the module directly which calls the         <code>demo/__main__.py</code> file.          <p>         This was a small reorganization and I like the result. In the past,         I usually just filled directories with python files, but I can         how this would have been useful for some larger projects.         I'll approach this like optimization: only do it when it becomes         necessary. I am reminded of the famous quote:          <blockquote>           Premature optimization is the root of all evil           <p>             - Donald Knuth           </p>         </blockquote>          <p>         I'll use modules once the individual files become a mess,         but stick to the files where I can.          <hr />          <p>         As of now, I am going to try posting here 3 days a week: Tuesday,         Thursday, and Saturday (or Sunday). I also want to write longer posts         on the main blog, but don't know how often I can get to that. I am         still going to try to write every day but not force myself to post         rushed writing.  
        ]]></description>
      </item>
    

      <item>
        <title>30</title>
        <link>http://ephjos.io/micro/2021/11/05</link>
        <guid>http://ephjos.io/micro/2021/11/05</guid>
        <pubDate>Fri, 05 Nov 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         My python ray tracer for the Ray Tracer Challenge has just about         caught up to where I was two years ago with my Haskell implementation.         Look, I'm back to phong shading!          <img           src="/img/demo_material.webp"           alt="phong shaded sphere"           width="480"           height="480">          <p>         I am feeling much more confident and I am actually enjoying working         through this book again. Now that this project is framed in a much         better way in my mind, I find myself looking forward to working         on it. This is a long way from the dread I felt the few times I         tried to pick it up again last year.          <p>         I also did a little cleanup of the code which was enjoyable, removing         unneeded static methods. The whole import tree is still needlessly         complex and some files export the demo functions for other functionality         in order to prevent circular imports. These should probably be broken         out into their own files, but it doesn't matter for now          <hr />          <p>         Wow, Day 30! I am honestly surprised that I did this, and even more         surprised that I enjoyed it overall. Some days were a total slog,         which I think is evident from the writing (especially the one where         I had to get myself out of bed at 11:30pm when I realized I forgot         that day's post). Most surprising was the amount of days that         writing these shorts posts seemed dreadful but turned into great         (or at least fun to write) posts.          <p>         From a standstill, the idea of writing often seemed like a lot of         boring work. But, once I got the ball rolling, I noticed I had a         whole lot of ideas that I wanted to write about. This whole effort         has had me reading, thinking, and daydreaming more. I still         want to reduce the amount of content I consume, since this blog has         illuminated just how much noise I have every day.          <p>         I am going to lax my constraint of writing here <i>every</i> day, but         I absolutely want to keep this habit up in some way. Maybe a weekly         goal works best, combined with a monthly goal for the main blog.         Tentatively, I'll commit to 3 micro posts a week and 1 blog post a         month. It'd probably be best to pick specific days and dates, but I'll         save that for later. Maybe I'll bring back the daily writing with         this year's         <a href="https://adventofcode.com/">Advent of Code</a>.          <p>         This has been a great experience and I can definitely say now that I         enjoy writing, at least in some capacity.  
        ]]></description>
      </item>
    

      <item>
        <title>29</title>
        <link>http://ephjos.io/micro/2021/11/04</link>
        <guid>http://ephjos.io/micro/2021/11/04</guid>
        <pubDate>Thu, 04 Nov 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         Great ideas are everywhere, the execution is what is rare.         Many a self help book has laid out the painfully simple steps to         get in shape or improve in one's career. Read through enough of them         and the overlap of ideas will become clear. It is not about reading         or having as many good ideas as possible, it is about consistently         executing on them.          <p>         I have read, watched, and listened to hours of media about time         management and creating focus. Finding a state of flow can be near         impossible, but it is an incredibly valuable place for a software         developer to be. I'd get inspired reading a book, break out a journal,         and start my daily schedule. Without fail, I would just stop after         a couple days and went right back to operating without a plan.         Like many other obstacles in my life, I noticed that I was feeling         a lot of friction when it came time to plan.          <p>         So, I am taking another         pass at things using a new piece of software that is much more         simple than what I've used before and is easier to work with         (and access) than a paper journal. I'll be taking a simplified         approach, focusing on an unordered list of tasks and the value         of the one in progress. When something on the list obviously         surpasses the current task in value, I'll switch my focus.          <p>         I'll update back here with the results after a month. Here we go again.  
        ]]></description>
      </item>
    

      <item>
        <title>28</title>
        <link>http://ephjos.io/micro/2021/11/03</link>
        <guid>http://ephjos.io/micro/2021/11/03</guid>
        <pubDate>Wed, 03 Nov 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I had a realization today that I <i>think</i> I may be able to get         my old Haskell ray tracer up and working. As it turns out, I may have         been completely wrong when expecting the program to be able to         generate high-resolution images. The implementation obviously has its         flaws, however I think I was simply expecting too much of it. This         clicked when I realized that the book suggests using much lower         resolutions than I expected when rendering. Even for a completely         blank canvas, the design of the program will lead to high memory usage.         This usage quickly exceeds what is feasible, aside from other         implementation details.          <p>         I am going to max out my python implementation at 640x480 images and         try to finish the book. If I find myself inspired, I'll loop         back to my Haskell implementation (still hidden in my git history)         and try the same there.          <p>         I don't know why this never hit me before, but I'll chalk it up to         the Covid outbreak starting while I worked on this. That sounds like         a good enough excuse to me :)          <p>         The affect that perspective and time spent away from a problem         can have on one's perception of the problem will always surprise me.         This seems dead simple to me now.  
        ]]></description>
      </item>
    

      <item>
        <title>27</title>
        <link>http://ephjos.io/micro/2021/11/02</link>
        <guid>http://ephjos.io/micro/2021/11/02</guid>
        <pubDate>Tue, 02 Nov 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         python is almost always a pleasure to use. I find that is helps         drastically lower the barrier between my brain and the computer,         letting ideas quickly become code without much friction. While this         may not be great for maintainability long-term, it is         great for "just doing" something which is often my goal. I like         using type systems and compiled languages, but I think python is         a better choice in most cases as it lets me focus on the problem at         hand without much effort. I should use python more often, especially         for pet projects.          <p>         Use the right tool for the job.  
        ]]></description>
      </item>
    

      <item>
        <title>26</title>
        <link>http://ephjos.io/micro/2021/11/01</link>
        <guid>http://ephjos.io/micro/2021/11/01</guid>
        <pubDate>Mon, 01 Nov 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         Sometimes, I find myself feeling deep appreciation for         well-designed libraries. This happens every time I have to write         my own matrix manipulation code. The actual mathematics is         not too difficult, and neither is the implementation (optimization         aside), but making a good interface is a pain. I will always         appreciate numpy and how seamless it is to use the ndarray.         The entire library is a great example of an extremely         performant backend and a stupid simple interface. Making such         a powerful tool so easy to use will always impress and inspire me.          <p>         I like numpy :)  
        ]]></description>
      </item>
    

      <item>
        <title>25</title>
        <link>http://ephjos.io/micro/2021/10/31</link>
        <guid>http://ephjos.io/micro/2021/10/31</guid>
        <pubDate>Sun, 31 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         From what I have read, it seems to me that the most successful people         in any field are those who are able to consistently have         deeply valuable bursts. They are not the people who slog away working         80 hour weeks for the sake of working more. Rather, they work long         hours when it will produce the desired result. Cal Newport's         <a href="https://www.calnewport.com/books/deep-work/">           Deep Work</a> touches on this idea a lot, and I can see this idea         echoed elsewhere.          <p>         It is not about doing more work, it is about doing         better work more often. This meta skill seems to be almost more         important than the "core" skill itself. For a given field, a competent         person who can routinely "go deep" will likely have better results         than someone who is better at the core skill.         After a certain level, it is not about knowing more, but about being         able to apply what you know more deeply and more often.          <hr />          <p>         Happy Halloween!  
        ]]></description>
      </item>
    

      <item>
        <title>24</title>
        <link>http://ephjos.io/micro/2021/10/30</link>
        <guid>http://ephjos.io/micro/2021/10/30</guid>
        <pubDate>Sat, 30 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I am switching back to python and trying to finish the Ray         Tracer Challenge. Had I not been detoured by different languages         and ideas, I would have been done this project at the start of Covid.         This being unfinished bothers me, so I am going to finish it. I am         not enjoying the experience as much because of this, so I hope I         can just get it done and then maybe circle back in the future to         learn new languages and tools.  
        ]]></description>
      </item>
    

      <item>
        <title>23</title>
        <link>http://ephjos.io/micro/2021/10/29</link>
        <guid>http://ephjos.io/micro/2021/10/29</guid>
        <pubDate>Fri, 29 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         As someone from the Northeast Philadelphia area, I grew up hearing         people say things like "Keep that up and they'll have to put you         in Byberry". My whole life, Byberry was just an abandoned         place where they used to send "crazy" people. It wasn't until today,         when I stumbled onto         <a href="https://allthatsinteresting.com/byberry-mental-hospital">           an article about Byberry Mental hospital</a> that I got more         detail on what actually happened there. The details in the article         paint a revolting picture of a system that completely failed those         who needed its help the most.          <p>         I have been interested         in the history of asylums and mental health care in 20th century         America since I went to the Pennhurst Asylum haunted attraction at         <a href="https://en.wikipedia.org/wiki/Pennhurst_State_School_and_Hospital">           Pennhurst State School</a>. I know many people find attractions like         this disrespectful, but I can say that it was a catalyst to education         for me. I would have never learned about the reality of what happened         at Pennhurst, Byberry, and the whole United States were it not for         this attraction. As a whole, this effort to help the mentally ill         resulted in an unquantifiable amount of damage done instead. Lives were         lost, innocent people suffered, and those responsible got away without         any punishment.          <p>         When reading about places and atrocities like this, I can't shake the         feeling that there is something similar going on today. Something that         we are doing that is so obviously abhorrent that people will look on us         with disgust in 50 years. In retrospect, these mental hospitals were         clearly failures on every level. But, why could people not see it back         then? How and why could they choose <i>not</i> to see it?          <p>         I think of Moriz Scheyer and something he said in         <a href="https://en.wikipedia.org/wiki/Moriz_Scheyer#Asylum">           Asylum</a>, written about his escape of the Nazis as an Austrian         Jew. During the occupation of France, Scheyer describe how every         non-Jew did everything that they could to make money. Even the Nazi         guards themselves would allow anything, should they be bribed with         enough money. Scheyer gives the reader the sense that the majority of         people were primarily motivated by greed and lust, either lying to         themselves about or actually oblivious to the true cause of the Nazis.         Wealthy French aristocrats threw themselves at the Nazis, just to         be aligned with those in power         <b>as they took their country by force and killed their fellow         men, women, and children</b>.          <p>         What in people allows them to ignore such horrors for personal gain?         What are we ignoring now?          <p>         What am <b>I</b> ignoring now?  
        ]]></description>
      </item>
    

      <item>
        <title>22</title>
        <link>http://ephjos.io/micro/2021/10/28</link>
        <guid>http://ephjos.io/micro/2021/10/28</guid>
        <pubDate>Thu, 28 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I hate naming things. A name never feels  <i>right</i> to me. Whether         it is a variable name, a game, an API route, or a team; names rarely         come easy to me. Names can be too long, too short, too wordy, or just         plain wrong. There always feels like there is a better name out         there. One that encapsulates the idea perfectly and is just beyond my         fingertips.          <p>         The best way forward is probably to go with my gut and stick by my         decision.   
        ]]></description>
      </item>
    

      <item>
        <title>21</title>
        <link>http://ephjos.io/micro/2021/10/27</link>
        <guid>http://ephjos.io/micro/2021/10/27</guid>
        <pubDate>Wed, 27 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         It often amazes me how much of a problem the "feature creep"         phenomenon can be. I am a sucker for bells and whistles,         especially when it comes to adding them to what I am working on.         The allure         of adding on just one more thing because "I am already in this code         and it shouldn't take too long and the result will be cool" is         strong. Like most computer science students, I read the Mythical         Man Month and the whole idea of feature creep was hammered into my         skull. It makes sense when discussing a team struggling to meet         deadlines; of course you have to abandon pet feature ideas in order         to fulfill your end of the contract. However, we were never         given much guidance on avoiding feature creep when you are the         one in control of deadlines and the product.          <p>         I've often seen this problem discussed by solo game developers. Like         most things, a game rarely starts out as a fully formed idea. There         is generally no end goal that "when the game works like this, it is done".         Games and all other interesting creations start with an idea which         eventually becomes some concrete entity. On the road between idea         and finished game are a series of stops where the developer has to ask         "What next?". New features insert themselves into the game with ease,         since it is not well-defined. Before anyone catches on, the game         becomes an abomination of 36 different mechanics that do not         work well together, resulting in a lackluster final product.          <p>         Much like physical minimalism, only introduce new "things" when         they serve a direct purpose. The mug holds coffee and is used every day;         the database stores all of our information and is accessed every         time a user logs in. Introducing extraneous and unneeded features will         have an exponential negative affect on your project, and can ultimately         overshadow the core value.          <p>         Creating something is just as much about saying yes to ideas as it         is to saying no; in fact, creating is about saying no <b>more</b>         than saying yes.  
        ]]></description>
      </item>
    

      <item>
        <title>20</title>
        <link>http://ephjos.io/micro/2021/10/26</link>
        <guid>http://ephjos.io/micro/2021/10/26</guid>
        <pubDate>Tue, 26 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I came across a post entitled         <a href="https://leancrew.com/all-this/2021/10/how-many-thursdays/">           How many Thursdays?</a> on Hackernews the other day. While the whole post is a great read,         there was one bit that stuck out to me. To summarize, the post tackles         how to answer the question of whether or not a month has 4 or 5         Thursdays in it (using an iOS Shortcut). The post then starts into         the solution with this paragraph:          <blockquote>           Since every month has 28–31 days, we know that each month has           four weeks plus 0–3 "extra" days. If the date of the first Thursday           is less than or equal to the number of extra days, there will           be five Thursdays in that month.         </blockquote>          <p>         Re-framing problems like this is what I love about computer science,         and this is a great example of it. Instead of taking the problem         straight on and just counting the number of days, there is a simple         way to rephrase the problem that makes the answer trivial. Instead of         a loop that may iterate 31 times (gasp!) the answer is as simple as         two lookups and a comparison.          <p>         This is a textbook elegant solution to me, and I think it has more to         do with reframing the question than coming up with a clever answer.         It reminds me of the first time I was able to wrap my head around         the         <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">           Monty Hall problem</a>.          <p>         Instead of focusing on the probability that         you pick the prize, focus on the probability that you initially pick a         goat. If 2/3 of the time you pick a goat, and you are always shown one         goat, switching results in you picking the car 2/3 of the time.          <p>         I think it will always fascinate me how these slight shifts in         perspective and framing can have such profound effects.  
        ]]></description>
      </item>
    

      <item>
        <title>19</title>
        <link>http://ephjos.io/micro/2021/10/25</link>
        <guid>http://ephjos.io/micro/2021/10/25</guid>
        <pubDate>Mon, 25 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         Given all of the         <a href="https://us-cert.cisa.gov/ncas/current-activity/2021/10/22/malware-discovered-popular-npm-package-ua-parser-js">           recent fun with ua-parser-js         </a>, package managers have been on my mind. In a design class         in college, I was introduced to         <a href="https://en.wikipedia.org/wiki/TRIZ">           TRIZ         </a>. TRIZ is known in English as "The Theory of Inventive Problem         Solving" and was born in the Soviet Union. The most valuable aspect         of this methodology is the idea of looking at the absolute worst         possible design for the final system. A phone that explodes, a plane         that can't fly, or a vacuum that can't suck up dirt are the kinds of         inventions drummed up during these exercises. Let's create the worst         package manager possible!          <hr>          <p>         To start, it would be great if the package manager was serving a         language with no standard library and that must run on every type         computer that exists. This ensures that there are edge-cases and bugs         aplenty which means that we will "need" a whole bunch of packages         to accomplish simple things. (Bonus points if the language is         interpreted and dynamically typed).          <p>         It would be awesome if the package manager couldn't detect         vulnerabilities in packaged code. And, it'd be even better if         it reported         <a href="https://overreacted.io/npm-audit-broken-by-design/">           completely safe code as dangerous         </a>.          <p>         Complete mayhem would be caused if the package manager sometimes         decided to install different code than what was requested. Or, we         could allow anybody to edit any package and republish whenever they         like. These two combined would guarantee that a user has absolutely no         idea what code they are bringing in.         If our community decided to treat more dependencies as though they         don't have a cost, we could start to see absurdly large         dependency trees that further exacerbate the unknown code issue.          <p>         Allow packages to define arbitrary code to be executed at install-time         so that a publisher can run whatever code they want on user's machines.          <p>         We could use a convoluted system for versioning: out with Semantic         and in with Pseudorandom Versioning! Every time the package         is published, pick a random 32 bit unsigned int and call that the new         version! Provide no notion of version order, making upgrades impossible.         Instead, always get the latest version (the only one that is available).          <p>         Provide no interface to allow a publisher to take down specific package         versions and broadcast potential vulnerabilities to downstream         dependents.          <p>         A little cherry on top would be if the tooling for interfacing with the         package manager was written poorly, sometimes crashing and often eating         up computational resources for seemingly no reason. We could even slap         a crypto miner in there to make some money!          <p>         Why stop there? Why not starting mining all of a user's local data on         their machine? Surely somebody would pay good money for it!          <hr>          <p>         While this is all intentionally hyperbolic, it is somewhat grounded and         targeted at <code>npm</code>. I am not aiming to         paint <code>npm</code> as the worst         possible version of a package manager that could ever be written; rather         I want to share the areas where there is an alarming amount of overlap         in my view. <code>npm</code> has provided great value to a whole         ecosystem of developers and by extension, their users/stakeholders.         I think there is a lot to be learned from the current state of         <code>npm</code> and what steps can be taken to make it a better         package manager. Even if it does not last, we can take these lessons         and use them to build a better package manager.         I wouldn't be bold enough to assume that I can come up with the list         of things a new package manager must get right in order to be great         (especially without much thought), but I can easily spot the things         not to do.          <p>         My most immediate concern is the ability for packages to execute         arbitrary code on install. I don't see how this needs to be a feature,         and I can't drum up any convincing argument in favor of it. A package         manager should allow a developer to grab a bundle of code that they         can then use as they wish.          <p>         There is a happy middle ground between         package size and the average number of dependencies. As of now, most         packages are tiny which allows them to be easily reviewed and audited.         However, this pushes the complexity onto the dependency tree itself.         It is near impossible to review thousands of nested dependencies by         hand. At the same time, one monolithic library is subject to the whims         of whoever controls it. There is no complex tree to look through,         but now there are tens (if not hundreds) of thousands of lines of         code to review. Somewhere in the middle there is room for moderately         sized packages with a handful of dependencies that are also similarly         sized. Both the packages and dependency tree can be reviewed in         a reasonable amount of time and trusted.          <p>         I think everyone who uses <code>npm</code> would agree that this         could all be much better, but I don't see how we get there.  
        ]]></description>
      </item>
    

      <item>
        <title>18</title>
        <link>http://ephjos.io/micro/2021/10/24</link>
        <guid>http://ephjos.io/micro/2021/10/24</guid>
        <pubDate>Sun, 24 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         For as long as I have had some form of responsibility, I         have dreaded Sundays. I would always find myself becoming         stressed or upset at some point in the afternoon, feeling the         freedom of the weekend falling away. Eventually, Monday would         come and drag along with it classes, obligations, and eventually         work. My understanding is that this is a common way to feel about         the end of the weekend, and that most people attribute it to not         enjoying what Monday brings. I don't think this is the case, at         least for me.          <p>         I think this sense of dread comes from feeling like the weekend was         missed or not properly used. The times where I've felt this most         strongly were when I was not intentional with my time on the weekend.         Inversely, when I am intentional about my time on the weekend I find         myself looking forward to Monday more. This weekend was fairly action         packed and ended with a solid chunk of hours mindlessly relaxing; I am         excited to get back to work and solving problems tomorrow. I've also         felt this way after weekends spent doing absolutely nothing.         The problem comes from the weekend feeling like wasted potential,         not having been properly used. I feel best when I feel like I have         gotten something out of my time on the weekend.          <p>         Your enjoyment of Monday is a function of how you spent the weekend,         more than what Monday will bring. Being intentional with time in         the present prevents the future from "taking" that time away.  
        ]]></description>
      </item>
    

      <item>
        <title>17</title>
        <link>http://ephjos.io/micro/2021/10/23</link>
        <guid>http://ephjos.io/micro/2021/10/23</guid>
        <pubDate>Sat, 23 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I found the fantastic personal website of Derek Sivers yesterday.         Just from the         brief look I've taken around the website, I can tell that there is a         bunch more reading to be done.         While browsing I stumbled on to his post about         <a href="https://sive.rs/daydream">daydreaming</a>.         While short, I found it interesting as I've largely stopped daydreaming.         There is always some form of stimulus between computers, phones, and         TVs that there is no room for boredom. Even when I can get hooked on         a book, which feels like a productive use of time, I am still         <i>consuming</i> information. The final line of this post has been         bouncing around in my head since I read it:          <blockquote>           <b>We’ve all had plenty of input.</b>           It’s fun to let your mind direct its own entertainment.         </blockquote>          <p>         Less input, more output.          <hr />          <p>         And, since I don't think I've actually committed to it in writing, my         plan for these posts is this.          <ul>           <li>             I will post everyday for 30 days straight.           </li>           <li>             After that, there will be no fixed schedule but this is where             all of my shorter posts will live. The main blog is where all             longer form writing will go.           </li>         </ul>  
        ]]></description>
      </item>
    

      <item>
        <title>16</title>
        <link>http://ephjos.io/micro/2021/10/22</link>
        <guid>http://ephjos.io/micro/2021/10/22</guid>
        <pubDate>Fri, 22 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         This morning I came across an interesting post from Dan Luu on         <a href="https://danluu.com/look-stupid/">looking stupid</a>. Some of         the points brought up in this post resonate with me and I can         understand the arguments being made for "looking stupid". From my         perspective, this manifests most often due to asking extremely simple         (some may say stupid) questions. I've noticed that the brightest people         I have been around tend to ask alarmingly basic questions. Go to any         martial arts gym and you will find that the most experienced and         knowledgeable people in the room are the ones asking the most         questions. There are a few reasons why I think this is the case:          <ul>           <li>             Simple questions are easier to answer which allows the person asking             them to more efficiently gather information from others. Over time,             this has a powerful effect on building knowledge. The more             answers, the better.           </li>           <li>             Asking basic questions is the best way to understand why somebody             else does something. Directly asking "Why?" often doesn't             lead anywhere as even the best at something are not always             conscious of the why. It is easier to teach and share what             to do than why, as the "what" often leads to the "why".           </li>           <li>             Those that excel tend to be great students; they are able to             push down the part of them that wants to respond with "I know"             and ask the question anyway. Strangely enough, it seems             that it is possible to receive the same answer to the same question             on multiple occasions and find that the outcome is different in             each instance. A different context or the presence of new external             information can help an old answer provide new insight.           </li>           <li>             In many cases, things are much more simple than they can seem.             It can seem stupid to ask obvious questions, but there is often             an obvious answer. This prevents over-complicating simple things.           </li>         </ul>          <p>         As pointed out in the post above, if you are truly in a healthy learning         environment then looking stupid is nothing to worry about. In order to         learn, you have to put yourself out there and broadcast that you         may not know something (gasp!). Ask simple questions, especially if they seem         like they are "below" your level of knowledge. When everyone is bought         in, this leads to a free flow of information and builds trust         between those involved. This benefits the individual and the whole         alike, and is something that almost any organization can benefit from.         I think the post misses out on this aspect of "looking stupid"; it can         actually have great social value as well.          <p>         Ask simple questions and look stupid.  
        ]]></description>
      </item>
    

      <item>
        <title>15</title>
        <link>http://ephjos.io/micro/2021/10/21</link>
        <guid>http://ephjos.io/micro/2021/10/21</guid>
        <pubDate>Thu, 21 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         From my limited experience, over-communicating seems like the best         approach to default to. I find myself hesitating when feeling like I         need to ask a question. In order to ask the question comfortably, I         have to feel like I've done my work first. I hate the feeling of asking         first, only to be told the answer is trivial. However, I've also found         myself deep in a rabbit hole that I did not have to go down many times         before as well. If I were to instead err on the side of         over-communicating, I can quite easily correct back. Often, especially         in high-paced work environments, people can be missing the full picture.         By over sharing you can help prevent team members from being lost and         aid them in accomplishing their goals. Under sharing leads to         miscommunication which can have drastic consequences. Over sharing         can be a temporary inconvenience.          <p>         I was given some advice the other day:          <blockquote>           If you do not feel like you are over-communicating, then you are           not communicating enough.         </blockquote>          <p>         Over-communicate.  
        ]]></description>
      </item>
    

      <item>
        <title>14</title>
        <link>http://ephjos.io/micro/2021/10/20</link>
        <guid>http://ephjos.io/micro/2021/10/20</guid>
        <pubDate>Wed, 20 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         When working on a piece of code, I am trying to be more mindful of         assumptions. It is easy enough to write down my assumptions while         reading code. This helps me check myself, and makes it easy to ask a         teammate targeted questions that can be answered with relative ease.         What's more difficult is checking external assumptions. In conversation,         it can be tricky to navigate but useful to ask someone "Are you assuming         X?". When done correctly, this can help everybody involved get on the         same page and focus on the task at hand. We also have to watch out for         assumptions in code we are monitoring. Some of these can be obvious,         laid out in a comment. Much like listing your own assumptions, these         are easy enough to verify. The danger comes from implicit assumptions.          <p>         We've all read and written functions that make assumptions about the         parameters that they are passed. This is especially dangerous in a         dynamically typed language, where any parameter can be any value.         Static typing does not wholly prevent this problem as there may be         edge cases within a type that must be accounted for. A function can         verify that its parameter is a number, but if it divides by that number         without checking it to be non-zero, then there are still dangerous         assumptions in the code.          <p>         These dangers extend past parameters and into all logic within a         program. We make assumptions about data flow, execution order, data         models, etc. that are all core to what the code does. While some of         these are unavoidable (i.e. synchronous code executes in order) we must         be mindful of all internal and external assumptions. Err on the side         of over communicating about these assumptions to better help your         team understand the piece of code being discussed. Pay close attention         to the code you read and write and be aware of any assumptions made         within.          <p>         Always check assumptions.  
        ]]></description>
      </item>
    

      <item>
        <title>13</title>
        <link>http://ephjos.io/micro/2021/10/19</link>
        <guid>http://ephjos.io/micro/2021/10/19</guid>
        <pubDate>Tue, 19 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <img           alt="Compounding growth chart"           src="/img/compounding.webp"           width="600"           height="400">         <blockquote>           Compounding growth chart showing $100 monthly contribution           with 5% annual interest over 40 years, starting with 0 initial           investment.         </blockquote>          <p>         One of the first lessons you should learn once you become interested in         personal finance is that compounding interest is one of the most         powerful tools for building wealth. Over large time scales, small         compounding interest can have incredible effects. Getting 5% interest         on repeated savings can lead to doubling your money in about 30 years;         and lead to tripling it in 40. The gradual nature of this growth         means that gains are not apparent for the majority of the time. After         investing a significant amount of time and resources, compounding         interest starts to take over. In the above example, the 40th year of         interest provides the same gain as the first ~18 combined. Exponential         growth seems to be unintuitive and can seem insignificant, barely         greater than linear for most of the time.          <p>         I once heard the following puzzle that someone was using to help educate         people about compounding and exponential growth.          <blockquote>           There is a lake with nothing else but a single lily pad. Every day,           the amount of lily pads doubles. If it takes the lily pads 50           days to cover the entire lake, how long does it take them to cover           half of it?         </blockquote>          <p>         49 days, doubling on the last day to cover the whole lake. The         fascinating thing to me was how sudden this burst at the end is. At day         45, the lily pads only take up 3% of the lake's surface area. The growth         for these first 45 was slow and largely undetectable day-to-day. The         last 5 days see a jump from 3% to 100% coverage. After a quick search,         here is a resource with this same scenario from a University of Washington professor:         <a href="https://atmos.uw.edu/~dennis/211_Exponential_Growth.pdf">Information sheet on exponential growth</a>.          <p>         I believe that this pattern can be seen everywhere, especially in         software projects.         Small investments that can be made on a regular basis provide a         seemingly disproportionate return over time. I think most developers         have seen this on a shorter time scale, say a couple of months. First,         there are a couple months of PRs that fix bugs and refactor code         without providing much value to the end user. Then, seemingly out of         nowhere, there are a handful of PRs that have a cascading effect leading         to dozens of issues being closed. The reality is that <i>all</i> of the         work done in those few months is responsible for the fixes at the end,         just that there needed to be a significant investment before         the true return could be realized.          <p>         This is the core idea behind James Clear's         <a href="https://jamesclear.com/continuous-improvement">1% better every day</a>         idea, which was deeply profound to me the first time I came across it.          <p>         Small continuous improvements lead to compounding growth.  
        ]]></description>
      </item>
    

      <item>
        <title>12</title>
        <link>http://ephjos.io/micro/2021/10/18</link>
        <guid>http://ephjos.io/micro/2021/10/18</guid>
        <pubDate>Mon, 18 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         Today I was able to finish the canvas chapter of the Ray Tracer Challenge         in racket. I spend a good bit of time digging through the racket docs         on basic string building and was able to get everything working.         The usage of vectors for storing the pixel data feels fast, but         converting the         pixel data to the output (PPM) string is slow.         This is somewhat expected, but is still a point of concern.         There are two aspects of this process that slow it down: converting         the vector to a list and string concatenation. Ideally, I would         directly pull the data out of the vector and add it to buffer, which         could then be written to the output file. The current implementation         uses higher-level string functions that will likely crush performance         going forward. This should not bring things to a halt, and the output         should still always be generated. Regardless, I expect to be back in         this code in the near future.          <p>         If I was to do this in Java, I would pull the information directly out         of an <code>Array</code> and append it to a <code>StringBuilder</code>.         Then, the <code>StringBuilder</code> could write its contents to the         output file. This solution would be quick and easy to read. I should         be able to get similar performance out of racket, once I learn more.  
        ]]></description>
      </item>
    

      <item>
        <title>11</title>
        <link>http://ephjos.io/micro/2021/10/17</link>
        <guid>http://ephjos.io/micro/2021/10/17</guid>
        <pubDate>Sun, 17 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         Another slight mindset shift I am trying to make is to focus on         problems, not solutions. This idea is parroted around a lot especially         in online-entrepreneur-influencer circles, but I think it does have         some value. As someone who can obsess over what the "best"         solution to a problem is and make the entire process much more difficult         for         himself, this tweak may prove helpful. Instead of being occupied         purely with the solution, pour more time and effort into learning about         the problem. The classic questions of who, what, where, when, why, and         how are all of the tools needed here.          <p>         The core hypothesis behind this idea is that a good solution should be         evident from the problem, once you have enough data about the problem.         Instead of languishing about large-scale architecture mistakes and         refactors, focus on the specific issue at hand. If the original author         is still on the team, reach out to them for some information. A quick         slack message may illuminate something before unseen to you. Identify         exactly what the issue is, and try to keep this definition targeted.         Pinpoint exactly where in both the code and the product this issue         occurs. Enumerate every place where this is the case, which may be         helpful for testing at the end. Understand what decisions/mistakes led         to this issue being checked-in to the code base. List out all of the         different manners in which the issue can be caused. Is it reproducible?         Is it deterministic? Given all of this information on top of the code         itself, clearly explain how the issue occurs. From code decisions to         what manifests to the end user, you should be able to explain each step         along the way and what key pieces lead to the final result.          <p>         With this detailed of a view on the problem, a list of potential         solutions should become immediately clear. Rank these solutions and         get feedback from team members (if necessary) before moving forward with         the best choice. Don't treat this is as a binding agreement: more         information may become available while working on this fix that reveals         another solution as a better choice. If unconstrained by time, feel free         to switch to the new approach.          <p>         Instead of focusing on the solution, obsess over the problem. Gather         info and make an educated decision on how to best tackle the problem.         The end goal is to fix the issue, not to create an elegant solution.         If the investigation into the problem is done right, this should         happen largely on its own.          <p>         Well defined problems reveal elegant solutions.  
        ]]></description>
      </item>
    

      <item>
        <title>10</title>
        <link>http://ephjos.io/micro/2021/10/16</link>
        <guid>http://ephjos.io/micro/2021/10/16</guid>
        <pubDate>Sat, 16 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         When working on a large project, knowing whether or not a pull request         is "good" can be difficult. We can find ourselves in abstract         discussions of what the best approach should be or leaving review         that "maybe you could fix X too". While these conversations have value,         they are not pragmatic and don't directly lead to code changes and PRs         being merged.          <p>         I am working on having a mindset focused on incremental improvement.         If a PR makes an undeniable improvement to the code base and does not         introduce any bugs or debt, then it should         be merged. If every PR can provide some improvement and be turned around         quickly, a team can rapidly rid themselves of the problems they spend         so much time discussing.          <p>         It is hard to walk the line between "fix everything" and "make         the project better", but I think it can (and should) be done. Identify         the core issues, create PRs to fix them, and merge the PRs. The ideal         PR is likely         <i>small and targeted</i> instead of         <i>large and perfect</i>. Over time, the smaller PRs can be merged faster,         provide a smaller surface area for bugs, and approach the perfect solution.  
        ]]></description>
      </item>
    

      <item>
        <title>9</title>
        <link>http://ephjos.io/micro/2021/10/15</link>
        <guid>http://ephjos.io/micro/2021/10/15</guid>
        <pubDate>Fri, 15 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         After briefly getting into racket's vectors, I am becoming a bit         more hopeful of finishing the Ray Tracer Challenge in racket. They         are default mutable         and come with analogs for most of the standard list functions (map,         filter, etc.). While I like pure functional programming and understand         the value, mutability can provide great performance improvements         in some cases (like         ray tracing). I'll be using vectors in the canvas struct, which contains         an RGB tuple for each pixel in the image we are trying to generate.         I believe that the source of the issues I ran into in Haskell was precisely         writing pixels to the canvas. Instead of having to copy the canvas to         do writes, simply get a reference to the pixel data and update a color         at a specific location. No need to worry about passing around and using         as State monad (which I still have not figured out how to do), just update         the value in place. This is exciting!          <p>         On a related note, I can never figure out a naming for "mathematical         vectors" when using them alongside vector containers. I need to use         the standard         library's <code>make-vector</code> to create the pixel data container.         This meant that I needed to rename my 3D vector (and point since they         are both "tuples"). I've gone with <code>make-vector-3d</code> in         this case, but I'd rather call the container an <i>ArrayList</i>         instead.         Or, since "list" is the default container type         in racket, <i>Array</i> would work just fine and make sense. 
        ]]></description>
      </item>
    

      <item>
        <title>8</title>
        <link>http://ephjos.io/micro/2021/10/14</link>
        <guid>http://ephjos.io/micro/2021/10/14</guid>
        <pubDate>Thu, 14 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I love simple tools.          <p>         Reflecting on some of what I talked about         <a href="/micro/2021/10/13">yesterday</a>,         I realized something else about myself. While friction and         resistance can be crippling, having too much freedom can be just as         bad. I notice this particularly with tooling I use often. Bells and         whistles do a great job of distracting me, and I will sink a         disproportionate amount of time into something that is practically         insignificant.          <p>         I think this part of my personality is what drew me to Linux.         The opportunity for endless customization and control was exciting         and pulled me in. I have spent countless hours customizing different         dotfiles and settings, a lot of which I use less than once a month.         While this is great educational value in this level of curiosity,         it does sometimes get in the way.          <p>         I've found that by focusing on a small set of simple and extendable         tools, I can get myself to focus on the problems I am trying to solve         much easier. Now that I am familiar with these tools, I can use, tweak,         update, and fix them without worry and (largely) without distraction.         There are fewer moving parts, no batteries to charge, no oil to change:         just a hammer to swing at a problem. While I understand and still         feel the allure of cool screenshots and retro programs, I find I enjoy         using a computer more by keeping it simple.  
        ]]></description>
      </item>
    

      <item>
        <title>7</title>
        <link>http://ephjos.io/micro/2021/10/13</link>
        <guid>http://ephjos.io/micro/2021/10/13</guid>
        <pubDate>Wed, 13 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         Last year, in the first summer of the pandemic, I listened to James         Clear's         <a href="https://jamesclear.com/atomic-habits">Atomic Habits</a> on         audiobook during my workouts. I found the ideas discussed within         to be fascinating and their application to be more intriguing. Of all         of the ideas discussed in the book, the most well known is probably the         idea of habit chaining. The idea here is that it is near impossible to         take on a bunch of new habits simultaneously, especially when they are         all disconnected. Instead, it is easier to "attach" a habit to an         existing one. If you want to write every morning, and you already have         the habit of waking up and making a cup of coffee, you can try chaining         writing onto the existing habit. Instead of trying to "write a blog post         everyday", aim to "write for 10 minutes <i>after</i> making coffee". This is         a smaller habit that builds up your existing system. There is lower         friction when trying to start the new task, since you are riding on         the momentum of your existing habit. With coffee in hand, you are         already prepped for and 10 minutes of writing away from completing         the task. Once you are writing everyday, it is easy to increase the         amount of time spent writing since you already have the habit of starting.          <p>         Remove the need         for motivation and instead make what you want to do easy, and what         you don't want to do hard. Its easier to eat healthy if you only have         healthy foods in the house; it is hard to east unhealthy if you only         have healthy foods in the house. This resonates with me the most         since I tend to <i>over-perceive</i> the amount of friction involved         in tasks.          <p>         When not part of a habit, simple tasks can become daunting and take         me a while to decide to do. I find that by trying to see the         true level of friction, I can "motivate" myself to do these tasks much         easier. Instead of being worried about how complicated a new feature         may be and how long it will take to implement and all of the potential         bugs that may be introduced: just start. Version control is a great         tool which allows us to undo or fix changes with ease.          <p>         For me, habits are best built by lowering the cognitive effort necessary         to start them, increasing the friction of the opposite task, and spending         what cognitive effort is necessary to analyze the true friction of the         task. It is less about finding the motivation than it is about         overcoming this "resistance" towards all tasks. Given these traits,         I seem to function best by simplifying the tools and systems around         me. I've found that         <a href="https://en.wikipedia.org/wiki/KISS_principle">"Keep It Simple, Stupid"</a>         tends to work for me. 
        ]]></description>
      </item>
    

      <item>
        <title>6</title>
        <link>http://ephjos.io/micro/2021/10/12</link>
        <guid>http://ephjos.io/micro/2021/10/12</guid>
        <pubDate>Tue, 12 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         After some discussions about unit tests at work yesterday, I came across         this solid <a href="https://softwareengineering.stackexchange.com/a/356238">StackExchange</a>         post. It turns out my understanding of the ways that a unit test can be         brittle were fairly limited, meaning I missed a few of the bad ways to         write unit tests. In particular, I previously would add in additional         checks that "just made sure" for different values/functionality along         the way. Often, these checks were almost completely orthogonal to the target         of the unit test. I saw this as more stable (more tests = better)         not as more brittle. But, now I understand the perspective that         extraneous assertions can lead to tests failing even when their "core"         tests pass. A rule of thumb to try going forward: strive to have         only 1 assert per unit test. While this is admittedly impractical and         not always the best approach, I think it is the kind of correction         I need to make in my approach.          <p>         On a different note, Ethan Chlebowski's recipe for         <a href="https://www.ethanchlebowski.com/cooking-techniques-recipes/street-cart-chicken-amp-yellow-rice">halal cart style chicken and rice</a>         is fantastic. I didn't think it would be so easy to recreate a         freshman year favorite of mine at home. If I exercise control         by making the salad portion bigger, rice portion smaller, and using a touch less         sauce, this is actually a solid healthy meal.          <p>         The most important         contribution Ethan has made to my cooking is his mayo marinade technique         which is applied in the above recipe. Instead of oil, marinate your         meat in a seasoned mayo which is oil+egg+vinegar. This results in         more tender, juicer meat with the bonus of not having to worry about         putting fat in the pan: the meat brings enough fat to cook it from         the marinade. The usage of mayo here also provides incredible color         to the finished product, which is always great to have. 
        ]]></description>
      </item>
    

      <item>
        <title>5</title>
        <link>http://ephjos.io/micro/2021/10/11</link>
        <guid>http://ephjos.io/micro/2021/10/11</guid>
        <pubDate>Mon, 11 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         Thanks to Tyson Fury vs. Deontay Wilder 3 causing me to stay up until         1:30am Saturday night, I am beat this morning. It was well worth it.         I am hoping a good workout         and some caffeine will help before the real work of the day begins.          <p>         I have found myself spending more and more time reviewing PRs, which         has been a great experience. This doesn't feel like it is cutting into         my other work and is helping me keep up with my team and get a better         understanding of the project as a whole. By time-boxing it to an hour         or two a day, I can ensure that I have time to fulfill my personal         responsibilities while still being accountable to the team. To         me, the biggest benefit of PR review besides QA and bug mitigation         is the sharing of knowledge.          <p>         A good PR should cleanly explain what the changes are, why they were         made, and what issues still exist in the affected code or linger         around the edges. This gives the reviewer a clear picture of the         motivation, allowing them to more easily see if the code completes         its objectives. The why gives insight into that developer's         perspective on the project as a whole, the issue in specific, and         their general problem solving approach. This can be incredibly         insightful, especially as a relatively new engineer, as it provides         a clear window into how my colleagues see things. When done well,         this can be a direct transfer of skills and knowledge that allows         the reviewer to become a better developer themselves.          <p>         Finishing with         the remaining issues and other concerns helps share that developer's         perspective on what work still has to be done. This spreads         project specific information which can help eliminate borders in         knowledge. It is never good to have a part of the code that         "only X understands, go ask them". While levels of expertise and         understanding vary, everyone on the team should have a minimally         workable understanding of each part of the codebase. This makes         the team more resilient to outside events while again         pushing each member's boundaries and ultimately making them better         at what they do.          <p>         If a team can consistently open concise, well explained PRs         that provide meaningful information and definitive improvements to         their project, both the final product and the engineers themselves will         improve. Do this for a period of time and a deeply valuable product         and team can be built. To be kitschy:         <blockquote>           software development is about developing software as much as           it is about developing the people who create it         </blockquote>  
        ]]></description>
      </item>
    

      <item>
        <title>4</title>
        <link>http://ephjos.io/micro/2021/10/10</link>
        <guid>http://ephjos.io/micro/2021/10/10</guid>
        <pubDate>Sun, 10 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I've hit that point in implementing a ray tracer where I need to         start implementing structs that have 2D arrays. This is needed         for the canvas' pixels and eventually in the matrix implementation.         This is straightforward enough in an imperative language, but has been         a sticking point in the past.          <p>         After some brief research, Racket's mutable vectors seem easy enough to         use, and are probably exactly what I need in this case. I want to         try to avoid building up thunks in these areas of the code as that         can lead to a massive space leak (as I've learned in the past).          <p>         If Racket's vectors are anything like those from Haskell, I'll         most likely wind up falling back to Python just to finish this project.          <p>         Wish me luck. 
        ]]></description>
      </item>
    

      <item>
        <title>3</title>
        <link>http://ephjos.io/micro/2021/10/09</link>
        <guid>http://ephjos.io/micro/2021/10/09</guid>
        <pubDate>Sat, 09 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         I've always wanted to write a somewhat substantial program         using a Lisp. I was first introduced to the family of languages in         a programming languages course: good-old MIT Scheme. This was         such a foreign experience at the time both around tooling and the         language itself. This opened up a whole other world of programming         languages that led to me finding Haskell and Racket. However,         I've never written anything too complex with either (aside from the         <a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/sicp/book/node76.html">SICP metacircular evaluator</a>).          <p>         There has been an ongoing cycle of start-stop-delete with my attempts at         <a href="https://pragprog.com/titles/jbtracer/the-ray-tracer-challenge/">The Ray Tracer Challenge</a>,         which will warrant a more lengthy post. These days, I am working through         this book in Racket. Just in the first chapter I've learned a bit more         about the language and its tooling, and I look forward to the rest.          <p>         I am thinking about stopping and just crunching through the book in         Python to get myself all the way through it. Maybe a first pass with         relatively low friction will help me actually get through it using         the other languages I have in mind. So far focusing on reducing friction         and finsihing things has served me well, so maybe that's what it needed         here.          <p>         I also still need to bring over a couple of my old blog posts to this         version of the site. 
        ]]></description>
      </item>
    

      <item>
        <title>2</title>
        <link>http://ephjos.io/micro/2021/10/08</link>
        <guid>http://ephjos.io/micro/2021/10/08</guid>
        <pubDate>Fri, 08 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
                <p>         So about that whole "rss feed by hand" thing.          <p>         I spent some time this morning writing a couple bash scripts         that iterate over the files for both blogs and         generates each their own rss feed <i>and</i> index page. This greatly         reduces the amount of work necessary for each post, while also forcing         me to stay in this simple format.          <p>         Yes, I am looking at the inflexibility of a hacked together bash script         as a feature, not a bug in this case. I also committed the sin of         using regex to parse text out of HTML, <a href="https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454"         >but I think I'll be okay</a>.          <p>         All in all, I am pretty happy with how everything works right now. To         create a post I copy my template file to the new post, write it, and         run a bash script before pushing git and docker. As long as I follow         some simple formatting that my scripts expect, all should be well.          <p>         I also noticed some super strange caching behavior when testing the         site on my other devices yesterday, so I am pushing a change to stop         caching. These pages are small and lightweight enough that it shouldn't         matter, future me can worry about that. 
        ]]></description>
      </item>
    

      <item>
        <title>1</title>
        <link>http://ephjos.io/micro/2021/10/07</link>
        <guid>http://ephjos.io/micro/2021/10/07</guid>
        <pubDate>Thu, 07 Oct 2021 12:00:00 -0400</pubDate>
        <description><![CDATA[
              <p>       This is my first micro post. I am not really sure what this       will turn out to be, but I do want to see how far I can take it.       This will just be a place to jot down what's on my mind at       the time. Posts like projects or other more involved writing       will go in the main blog.        <p>       As of now, I am working on laying the website out again (for I       believe the 6th time). I am keeping the majority of the       UI/UX the same, but moving completely to a plain HTML/CSS/JS       stack.        <p>       I found out about SSI (Server Side Includes) the other day,       so I don't even need to use an external templating system.       Just set <code>ssi on</code> in nginx and get up and running.       This lets me share the head, navbar, and footer between pages       easily which was always my main deterrent from dropping all       the way down to bare HTML.        <p>       I've previously told myself that I don't like writing HTML and       that I <b>needed</b> a markdown setup in order to write, but       that certainly didn't lead to more writing.        <p>       So now, everything is dead simple. These posts don't       even really need markup, and in the cases that they do I'm       sure I'll get around to adding some useful snippets to my       vim config.        <p>       Creating the index pages and rss feed by hand is a bit strange,       but the whole theme of this effort is to <i>just do it</i>.        <p>       Here it goes! 
        ]]></description>
      </item>
    

</channel>
</rss>

