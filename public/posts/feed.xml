<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
   <title>ephjos posts</title>
   <description>posts</description>
   <language>en-us</language>
   <link>http://ephjos.io/posts/feed.xml</link>
   <atom:link href="http://ephjos.io/posts/feed.xml" rel="self" type="application/rss+xml" />


    <item>
      <title>Advent of Code 2022</title>
      <link>http://ephjos.io/posts/2022/12/01/index</link>
      <guid>http://ephjos.io/posts/2022/12/01/index</guid>
      <pubDate>Thu, 01 Dec 2022 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>           <a href="https://adventofcode.com/">Advent of Code is back</a>!           I'll be updating this post with my           thoughts on each day           throughout the challenge. The full source for my solutions           can be found           <a href="https://github.com/ephjos/aoc/tree/master/y2022">in this git repo</a>.           I was looking into using zig or c this year, but I am going to go           with python. I think this will be the fastest way for me to complete           each challenge, which is a priority for me this year.         </p>          <div>           <h3>Day 01</h3>           <p>             Today was a typical AoC Day 1. Part 1 was a simple string split on             <code>"\n\n"</code> and then a max of sums. Part 2 was sorting             these sums and returning the sum of the 3 largest. I'm happy to             be kicking things off again this year. As-is, I am pretty happy with             my template.           </p>         </div>         <div>           <h3>Day 02</h3>           <p>             These problems were straightforward, but my solution was a bit of             a mess. The key insight (I think) is to map the input chars to             their numbers and then perform arithmetic with modulus 3 on the             result. I think this would give the answers in the cleanest way             possible, but I know that I always struggle with 1-indexed             modulus, so I wrote out some extra if statements that I             was sure are correct. I managed to get ranks 1270 and 858             respectively, which I am happy about. Maybe I'll polish             this solution up in the morning.           </p>         </div>         <div>           <h3>Day 03</h3>           <p>             This was one of those days where I did not see an obvious easy             answer until I was working on the second part: use set             intersections. Once I realized this, I refactored my part 1 and             it seems to read a bit better. Also, I'm sure there is an easier             way to map from chars to "type", but I used a dict in my             implementation. I enjoyed these 2 problems.           </p>         </div>         <div>           <h3>Day 04</h3>           <p>             Another day where sets come to the rescue! I was initially going             to use a simple dataclass to track each range, but I realized             that sets may be a better choice. The first part was checking             for supersets and the second part was checking if the             intersection was non-null. I complicated things for myself             by insisting on using a list comprehension to parse the input,             which is something I don't like doing: but I got the problem             solved fairly quickly. Today was fun!           </p>         </div>         <div>           <h3>Day 05</h3>           <p>             This was a good problem, but I did not sort out the parsing             as fast as I would have liked. I went down the path of trying             to split the input strings from the start, when I should             have thought for a moment first: using direct string             indices works better. I figured we wouldn't have a scaling             issue in part 2 as this is only day 5, so I went ahead             and implemented the stacks as described in the             problem. For part 2, I pushed to a temporary stack             during the swap and then pushed to the destination pile in             reverse. I could have done this in a different way, but I was             confident I would get this approach correct faster.           </p>         </div>         <div>           <h3>Day 06</h3>           <p>             At first, I tried to do this a responsible way and track the value             of each character, returning early when the marker was found. After             a few minutes of issues with this, I went with the obvious             approach for me. This was to iterate over the whole input string             and check if every window of 4 characters was unique. I             did this by constructing a set and checking if the length was still             4. While this is likely not the best performance-wise, thanks to the             string slice and set construction, it was the easiest to write.             And today's part 2 was a simple extension of the problem, growing             the window from 4 to 14. Instead of challenging runtime,             this tweak challenges how easily adaptable the part 1 code was,             which I think is interesting. For me, I copy/pasted the part 1             answer and change the 4's to 14's: which was all I needed to do.             I enjoyed today's problems!           </p>         </div>         <div>           <h3>Day 07</h3>           <p>             Today was a bit of a struggle. The problem itself was not too             complex, I simply could not come up with a good data structure             to represent the problem. Days like these are the most             challenging for me. I ended restarting 30 minutes in and             cramming everything into nested dictionaries. I then used             a recursive walk to calculate the directory sizes. The             end instructions for the final sum were a bit confusing, but             I was eventually able to work them out. Thankfully, part 2             only modified the final calculation and did not             introduce a larger scale change. I need to brush up on my tree             representations and read about more elegant ways             to parse this input and represent it in memory.           </p>         </div>         <div>           <h3>Day 08</h3>           <p>             I thought today's problem was right near the sweet spot of             difficulty for me, given this is a weekday puzzle. The             instructions were a little opaque, but not impossible to             understand. The task itself had an obvious solution             for both parts, but brought in concerns about runtime.             I took the naive approach for both parts, performing the             scans of the grid as laid out in the question.             I'm confident that there is a much faster solution,             and I think dynamic programming may be possible.             I have no plans to look for such optimizations this year,             and I will certainly not be looking for them in my first             solution. I enjoyed today!           </p>         </div>         <div>           <h3>Day 09</h3>           <p>             Today was fun! I expected that the second part would either be             complicating the rules, but the longer rope was an interesting             surprise. I was able to refactor part 1 to be general in just             a few minutes, and get the part 2 answer without much trouble.             In part 1, I was tripped up and tried updating the head the             full amount of steps all at once. Once I realized my mistake, and             wrote out the core update code in a verbose way, I didn't have any             issues. This was a good problem!           </p>         </div>         <div>           <h3>Day 10</h3>           <p>             This was a hard day. One of the core challenges of AoC is             reading and understanding the questions. The best solvers             in the world can do this in seconds. I struggled to understand             today's question for ~40 minutes. I'm not quite sure if I can             articulate what I <i>thought</i> the question was asking.             I eventually deleted my code and re-read the prompt, and             was able to hack something together for both parts.             I have no idea what a good answer would look like, but I can             guarantee that my solution is not it. I need to caffeine.           </p>         </div>         <div>           <h3>Day 11</h3>           <p>             Today was not fun. There was a lot of careful reading required.             I wrote a bunch of boilerplate to represent the problem clearly.             I then go hung up on some weird python issues; lambdas seemed             to be changing the value they would return. There was a             lot of bad code written today. Then, I could not figure out part 2.             I knew modulo would be involved, but I could not see that we             needed the LCM of the divisors. If I had noticed that they were             prime numbers sooner I may have realized that I needed to             multiply them. I need to brush up on my modular arithmetic.           </p>         </div>         <div>           <h3>Day 12</h3>           <p>             I am struggling with reading comprehension after just waking up.             Recognizing the solution and coding it up was trivial, I             wrote both BFS and Dijkstra's. I could not             figure out what was wrong. The first gotcha was that elves can jump             down any height without issue, so only neighbors more than one             higher the current position had to be excluded. The second gotcha was             the values to assign to S and E. I mistakenly chose -1 and 26, when             the problem clearly states they should be the same as a and z (0             and 25). I explicitly looked for this information at the start,             and missed it. I wasted most of the time today looking for             this information again. The example input does not have a y adjacent             to E, so this case is not tested; the z must be visited first             anyway.           </p>           <p>             I desperately need to find a better time to work on these             problems, because I am creating all kinds of unnecessary             complications when trying to do them half-asleep.           </p>         </div>         <div>           <h3>Day 13</h3>           <p>             Today was fun! I used python's <code>eval</code> to parse the             list strings into python lists directly. I was stuck for a while             returning bools, when I realized that I needed to write a             true <code>cmp</code> style function, returning -1, 0, or 1 instead.             This lead perfectly into part 2, where we had to use such a             comparison function to sort all of the packets. I was concerned             for a moment, but then I found             <a href="https://docs.python.org/3/library/functools.html#functools.cmp_to_key">functools.cmp_to_key</a>,             which allows you to pass a <code>cmp</code> style function             to the <code>key</code> argument of <code>sorted</code>. This             made everything straightforward and I got my part 2             answer without issue. Taking a few more minutes to wake up             before starting this morning was a good decision.           </p>         </div>         <div>           <h3>Day 14</h3>           <p>             This was a good problem. I again took some extra time to wake up             this morning and read the problem slowly. I implemented             everything as the problem laid out, and expected either a scaling             or a rule complication in part 2. We were lucky that today did not             require major changes to the naive approach in part 2. I             enjoyed this problem, and I am looking forward to seeing             what other solutions looked like. I have a hunch that there is             much more efficient way to simulate the sand (without simulating             it directly), but have not put much thought into it.           </p>         </div>         <div>           <h3>Day 15</h3>           <p>             Today was fun. I figured we would have scaling issues, given the             size of the input numbers. Part 1 was fairly straightforward,             the only hangup being what bounds to use when checking the row.             Part 2 was hard, but it was fun. I realized that the solution must             be 1 step off of the border of one of the coverage regions. So I             took the most naive approach which was to add all of these             eligible values just off the borders and check each one.             The solution is slow, but it takes ~1 minute on my machine,             which I am happy with. Outside of SAT solvers and other hyper             aggressive solutions, I'm sure there is much performance to             be gained with my approach as it wastes a bunch of work. Instead             of using an intermediary set, I could check points as I go for             example. I enjoyed this problem!           </p>         </div>         <div>           <h3>Day 16</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 17</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 18</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 19</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 20</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 21</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 22</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 23</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 24</h3>           <p>Notes go here...</p>         </div>         <div>           <h3>Day 25</h3>           <p>Notes go here...</p>         </div> 
      ]]></description>
    </item>
  

    <item>
      <title>website tweaks</title>
      <link>http://ephjos.io/posts/2022/11/30/index</link>
      <guid>http://ephjos.io/posts/2022/11/30/index</guid>
      <pubDate>Wed, 30 Nov 2022 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>           I pushed some changes to this site yesterday:           <ul>             <li>Moved everything from <code>/blog</code> and <code>/micro</code> under <code>/posts</code></li>             <li>Consolidated everything to a single RSS feed</li>             <li>Upgraded to redbean version 2.2</li>             <li>Renewed the SSL certs</li>             <li>Killed unneeded processes on the host</li>             <li>Added DDoS protection using <a href="https://redbean.dev/#tools">blackholed.com</a></li>             <li>Pulled all configuration into the repo and made deployment more hermetic</li>             <li>Run redbean with strict sandboxing (<code>-SSS</code>)</li>           </ul>         </p>          <p>           As always, the goal is to make the process of writing as simple as possible here,           and I feel that these changes are steps in that direction. Its been           great to watch the ecosystem around redbean slowly grow, and I am           excited to see where it goes in the future. For now, the host machine           this server runs on no longer runs nginx, fail2ban, or docker; which           is pretty cool.         </p> 
      ]]></description>
    </item>
  

    <item>
      <title>kilo</title>
      <link>http://ephjos.io/posts/2022/11/29/index</link>
      <guid>http://ephjos.io/posts/2022/11/29/index</guid>
      <pubDate>Tue, 29 Nov 2022 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>           I recently went through the           <a             href="https://viewsourcecode.org/snaptoken/kilo/"           >Build Your Own Text Editor</a>           tutorial. This booklet goes over building a simple terminal-based           text editor in C. The editor itself has no dependencies (outside of           the standard library) and is supposed to be decently portable.           The editor itself is           <a             href="http://antirez.com/news/108"           >antirez's kilo</a>.         </p>          <p>           I thought that the project was interesting and definitely provided           a large amount of useful information. The most valuable aspect           of these tutorials for me is seeing how someone else writes software.           I'd read the description of a section and have a rough idea of what           the implementation would look like; only to have the author do           something different.           Sometimes, I see patterns that I do not want to implement myself.           Other times I find new ways of doing things that I will adopt           myself.           In traditional CS and programming education we spend the bulk of our           time in the abstract. Being able to dive into concrete examples           provides a different but still useful experience.         </p>          <p>           For this tutorial, I was surprised by the high-level architecture           with respect to rendering. The core rendering code looks like it was           pulled straight from a game engine, being written in an immediate           mode style. This makes sense, given the context of this being           a terminal application, but the similarity to game code was a           surprise nonetheless. I also got to see a different approach           to strings in C than I've seen before. I'll be looking to brush           up on my <code>memset</code> skills thanks to this tutorial.           I also gained some more appreciation for the curses libraries,           given how difficult it is to interact with the terminal without them.         </p>          <p>           All-in-all, I'd recommend going through this tutorial. It was           enjoyable throughout and has given me some ideas I want to play           with some more. For languages like C, where there is no single "way",           reading other people's code provides a valuable perspective.         </p>  
      ]]></description>
    </item>
  

    <item>
      <title>tic-tac-toe bitboards</title>
      <link>http://ephjos.io/posts/2022/08/07/index</link>
      <guid>http://ephjos.io/posts/2022/08/07/index</guid>
      <pubDate>Sun, 07 Aug 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>           While checking out the           <a href="https://github.com/jart/cosmopolitan/blob/master/tool/viz/life.c">             source code for the Game of Life implementation in cosmopolitan libc</a>           I was introduced to the concept of bitboards. I was aware of using bit-level           logic for performance and memory reasons, but I did not know that there           was a specific name for applying this to games. The cosmopolitan source calls           out           <a href="https://core.ac.uk/download/pdf/33500946.pdf">             Bitboard Methods for Games by Cameron Browne</a>, which is           a great resource. It is clear to me how this approach can be used to           implement Conway's Game of Life and other grid based games. The obvious way           to improve such systems is to parallelize the computation across the           board. I've gone the GPU accelerated route before but it never           occurred to me that a similar result can be accomplished using bit-wise           operations.         </p>         <p>           As an exercise in understanding, I wanted to implement something not           explicitly called out in the above paper. I went with tic-tac-toe,           since it is simple enough to do in short amount of time.           <a href="https://github.com/ephjos/ttt-bitboard">             I implemented something in C</a>, and I am happy with how it went.           I am not thrilled with the board printing logic, it feels like there           should be an easier way to map an int to a string of output without           using a mask for each bit. I'm sure that I could do away with the usage           of pointers and replace some of the conditionals, but I don't know how           much is left to be gained there.           While not groundbreaking, this was interesting and I would like to look           at applying this to more complex problems starting with cellular automata.         </p>         <p>           When I first started programming, I remember implementing tic-tac-toe           in Java. I used 2D arrays (tic-tac-toe is probably a great introduction           to this idea, but is far from optimal) and had plenty of loops           to check win conditions. The input handling was janky and the output           was a mess. I spent a lot of time writing logic to make the class           general, which was definitely a waste of time. This seems to be a common           issue, especially in Object Oriented code, but that is a topic for           another day. While it is nothing fancy, I know myself from Tenth grade           would think that this is cool.         </p> 
      ]]></description>
    </item>
  

    <item>
      <title>redbean</title>
      <link>http://ephjos.io/posts/2022/06/18/index</link>
      <guid>http://ephjos.io/posts/2022/06/18/index</guid>
      <pubDate>Sat, 18 Jun 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>           This site now runs on           <a href="https://redbean.dev/">             redbean</a>! I remember first coming across           <a href="https://news.ycombinator.com/item?id=26271117">             a post on HackerNews</a> last year and being fascinated.           With the recent release of version 2, I knew I had to move this           site over.         </p>         <p>           For all of the effort I put into keeping the site small and simple,           it always felt weird to deploy it using docker behind nginx. I           initially wanted to use a smaller docker image, but that felt like I           was avoiding the problem. Now, this website is a single           zipfile/executable (on the order of a few MB) that can be run on           Linux, Mac, Windows, FreeBSD, OpenBSD, and NetBSD. To deploy,           I rsync the file to a server and ssh in to restart the server.           The only infrastructure needed on the server side is setting up           some files paths, initializing SSL certs using certbot           from Let's Encrypt, and dropping in a script to use the certs.         </p>         <p>           I was able to set everything up by using the redbean docs without           issue.           The only major quirk I ran into was with caching, but it looks like           <a href="https://news.ycombinator.com/item?id=31769195">             this is a known issue</a>. I got around this by not using caching,           which is something I don't mind doing since I intentionally           try to keep this website small.         </p>         <p>           I'd love to spend some time to wrap my head around           <a href="https://github.com/jart/cosmopolitan">             Cosmopolitan Libc</a> and create some           <a href="https://justine.lol/ape.html">             Actually Portable Executables</a> of my own.         </p> 
      ]]></description>
    </item>
  

    <item>
      <title>limits</title>
      <link>http://ephjos.io/posts/2022/06/09/index</link>
      <guid>http://ephjos.io/posts/2022/06/09/index</guid>
      <pubDate>Thu, 09 Jun 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>           On a recent episode of the Tim Ferris Show           podcast, Tim Ferris had a conversation with Morgan Housel whose work           <a href="/posts/2021/11/28">has inspired me before</a>. I would           recommend the episode overall, but there was one small observation           from Morgan that has been stuck in my head           (<a href="https://tim.blog/2022/03/05/morgan-housel-the-psychology-of-money-transcript/">full transcript</a>):         </p>          <blockquote> ... there’s a friend of mine named Carl Richards, who’s a financial advisor, and he has a quote where he says, “Risk is what is left over when you think you’ve thought of everything.” And I think that’s the definition of risk is whenever we’re done planning and forecasting, everything that’s left over that we haven’t thought about, that’s what risk actually is. And the takeaway from that, the actual practical takeaway is that if you are only planning for risks that you can think about and you can envision and you can imagine, then 10 times out of 10, you’re going to miss the biggest risk that actually hits you.         </blockquote>          <p>           As with a lot of Morgan's insight this was presented in the context of           finance and investing. However, his observations often apply in a           much more general context. When I heard this, my mind immediately           went to how I look at building software.         </p>          <p>           I've often had the problem where I start working on a new idea and           make decent progress, only to run into a snag that pokes holes in           my overall approach. At this point, I'd have to start back over with           my new perspective and take another crack at the problem. I had           always wondered why this happened, and what I could do to mitigate it.           I'd agonize about potential architectures and approaches that could           best solve the problem and find myself reluctant to start for fear           of taking the wrong approach. Morgan's observation is that this           is not something to fear, and is something that is hard to prevent.           It is precisely the things that you don't account for that will           trip you up, and it is nearly impossible to take <i>everything</i>           into account.         </p>          <hr />          <p>           A thread on HackerNews put me onto a few great talks, all starting           with           <a href="https://www.youtube.com/watch?v=pW-SOdj4Kkk">             Preventing the Collapse of Civilization by Jonathan Blow           </a>. This is a great talk in its own right, but for the purpose           of this post it served as the springboard into           <a href="https://www.youtube.com/watch?v=5IUj1EZwpJY">             The Only Unbreakable Law by Casey Muratori           </a>. In this talk, Casey builds up to and elaborates on something           known as           <a href="https://en.wikipedia.org/wiki/Conway%27s_law">             Conway's Law           </a>. This law states that organizations will produce output whose           structure is a copy of the organization's. Casey provides a few           great examples, most clearly how the structure of the Windows team           at Microsoft influenced the final operating system. The existence           of separate Direct Audio and Direct Input teams resulted in there           being two distinct software packages for handling audio and input.           These libraries, along with others, were wrapped under the DirectX           API, which was the overall product of the department           that contained all of the Direct teams.         </p>          <p>           There are a few core ideas here, but the one that clicked for me was           the idea that by defining a structure of organization, the           organization is inherently limited in the space of output it can           produce. From the Wikipedia page above, "If you have four groups           working on a compiler, you'll get a 4-pass compiler." In other words,           four groups working on a compiler are unable to make a 1-pass           compiler. There may be areas where it would make sense for the teams           to collaborate and avoid repeating work or find optimizations, but           the communication           costs for the 4 teams is too great. In order to coordinate that many           people on the project, they have to be put on teams. The way teams           are broken up will directly influence the shape of the output.         </p>          <p>           An organization, by its construction, may be incapable of producing           the best solution for the problem that the organization was created           to solve. However, some of these "optimal" solutions may not be able           to be formed into an organization. A single person can only hold           a certain amount of information in their head. There will be a point           where responsibility must be divided. Where this division is drawn           will directly impact the amount of possible outputs possible. These           observations apply to any organization, be it a company or the           layout of code.         </p>          <hr />          <p>           There are limits on what we can plan for. Trying to identify all           of the risks directly           <a href="https://en.wikipedia.org/wiki/Uncertainty_principle">             changes what our risks are</a>. We don't know what the best           solution to a problem is           before we find it. The way we organize our approach to the           problem directly restricts the set of solutions we can find.         </p>          <p>           Don't obsess over planning, focus on doing. Organize only where           absolutely necessary.         </p>  
      ]]></description>
    </item>
  

    <item>
      <title>dolly zoom</title>
      <link>http://ephjos.io/posts/2022/04/13/index</link>
      <guid>http://ephjos.io/posts/2022/04/13/index</guid>
      <pubDate>Wed, 13 Apr 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>           Tinkering more with           <a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">             Ray Tracing In One Weekend</a>, I managed to render a dolly zoom.           The           <a href="https://en.wikipedia.org/wiki/Dolly_zoom">             Dolly Zoom Wikipedia page</a> has an equation for calculating           distance as a function of FOV, which made this straightforward.           I solved the equation for FOV instead, and then rendered a bunch of           frames while moving the camera closer to the center ball.         </p>          <video autoplay muted loop controls>             <source src="/vid/rtiow_dolly_00.mp4" type="video/mp4">             Cannot render video in your browser         </video>  
      ]]></description>
    </item>
  

    <item>
      <title>ray tracing in another weekend</title>
      <link>http://ephjos.io/posts/2022/04/11/index</link>
      <guid>http://ephjos.io/posts/2022/04/11/index</guid>
      <pubDate>Mon, 11 Apr 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>           I'm still playing around with           <a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">             Ray Tracing In One Weekend</a>, this time by re-writing it in C.           I wanted to implement this on CPU and play around with a little more           than what my WebGL implementation had to offer. This way, I can           do stuff like render a bunch of frames in a for loop and use ffmpeg           to glue them together into a video (with a cool           <a href="https://superuser.com/a/1608376">             boomerang effect</a> too)!         </p>          <video autoplay muted loop controls>             <source src="/vid/rtiow_00.mp4" type="video/mp4">             Cannot render video in your browser         </video>          <p>           I've got some code cleanup and performance improvements to take care           of, and then I'll move on to           <a href="https://raytracing.github.io/books/RayTracingTheNextWeek.html">             The Next Week</a>.         </p>  
      ]]></description>
    </item>
  

    <item>
      <title>ray tracing in a weekend</title>
      <link>http://ephjos.io/posts/2022/04/03/index</link>
      <guid>http://ephjos.io/posts/2022/04/03/index</guid>
      <pubDate>Sun, 03 Apr 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>           Welp, everything outlined in           <a href="/posts/2022/01/28">habits</a> definitely did not happen.           Something something best laid plans of mice and men.         </p>          <p>           I have been distracted with a few things, mostly playing around with           WebGL. I was pulled down the           <a href="https://www.shadertoy.com/">ShaderToy</a> rabbit hole and           have been playing around with all kinds of ideas. Most notably, I           implemented the           <a href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">             Ray Tracing in One Weekend</a> project in a fragment shader, which           you will see below. I learned quite a bit about WebGL, passing data           to shaders, and pseudo-random number generation in shaders. The below           example uses a fixed canvas size of 256x256, stretched with CSS. There           are 73 balls placed randomly in the scene each time that the shader           is compiled, which happens every page load. All of the objects are           written as constant structs into the shader, and the main loop           is unrolled. The compiled shader is           logged to the console. Each ray is allowed to bounce 50 times. Also,           this appears to be a path tracer (to my understanding), since we are           taking random rays when we bounce, instead of the exact reflection.           I implemented some basic WebGL abstractions           and the progressive rendering using some of the logic in           <a href="https://github.com/evanw/webgl-path-tracing">             evanw's webgl-path-tracing</a>.         </p>          <div class="container">           <canvas id="glcanvas" width="256" height="256"></canvas>           <div id="overlay">             <div id="size"></div>             <div id="fps"></div>             <div id="frame"></div>           </div>           <div></div>         </div>          <p>           I also found           <a href="https://github.com/sschoenholz/WebGL-Raytracer">             sschoenholz's WebGL-Raytracer</a> which does some cool texture           packing to pass the scene information to the shader.         </p>          <p>           All-in-all, I learned quite a bit about WebGL, raytracing, and path           tracing which makes me interested in doing more. I am not interested in           trying to add anymore complex features to my existing python           implementation of the Ray Tracer Challenge, so I think it is           time for a rewrite in a lower-level language.         </p>          <hr />          While I have been better with using my phone less (thanks to locking         myself out from the internet using         <a href="https://freedom.to/">Freedom</a>), I still need to         get back in the groove with reading and writing.  
      ]]></description>
    </item>
  

    <item>
      <title>monty hall</title>
      <link>http://ephjos.io/posts/2022/03/15/index</link>
      <guid>http://ephjos.io/posts/2022/03/15/index</guid>
      <pubDate>Tue, 15 Mar 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I watched a great video from 3Blue1Brown today, and it reminded me         of what it was like to first wrap my head around the Monty Hall problem         as a CS undergrad. The video,         <a href="https://www.youtube.com/watch?v=OkmNXy7er84">The Hardest Problem on the Hardest Test</a>,         is about a question from a Putnam Competition. Finding the solution is         a matter of reframing the problem and changing perspective. Once viewed         from the correct lens, the answer seems trivial.         </p>          <p>           From           <a href="/posts/2021/10/26">my post on 10/26/2021</a> when           discussing the           <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">             Monty Hall problem</a>.            <blockquote>             Instead of focusing on the probability that             you pick the prize, focus on the probability that you initially pick a             goat. If 2/3 of the time you pick a goat, and you are always shown one             goat, switching results in you picking the car 2/3 of the time.           </blockquote>         </p>          <p>           I know I've discussed this before on this blog, but this skill is           something that I have been working to develop ever since exposed to           it. In CS, we spend a ton of work on formulating efficient data           structures and algorithms and then reframe the problem to fit           these abstractions. This gets to the core of what I find so           interesting about computer science, and seeing another example           of it in the wild is always intriguing.         </p>  
      ]]></description>
    </item>
  

    <item>
      <title>pi day</title>
      <link>http://ephjos.io/posts/2022/03/14/index</link>
      <guid>http://ephjos.io/posts/2022/03/14/index</guid>
      <pubDate>Mon, 14 Mar 2022 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>           In celebration of Pi Day, I figured that I could get around to           implementing a simulation of           <a href="https://en.wikipedia.org/wiki/Buffon%27s_needle_problem">             Buffon's Needle</a> for approximating pi. I've implemented           this in client-side JS and used WebGL for rendering. It's been           a while since I used WebGL directly and wrote some vanilla JS.           The           <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API/Tutorial/Using_shaders_to_apply_color_in_WebGL">             Mozilla docs for WebGL</a> have tons of great examples that           can be easily used as a jumping off point. This was a           fun exercise, and it was interesting to work with this first hand.         </p>         <p>           The implementation is far from optimal and does all of the heavy           lifting in JS. This could clearly be made more efficient, which           will be left as an exercise for the reader. Play with the parameters           and see how long it takes to generate a basic approximation of pi!         </p>         <div class="container">           <canvas id="glcanvas"></canvas>           <div id="overlay"></div>           <div>             <label>               Lines:               <input id="lines-slider" type="range" min="2" max="20" value="5" step="1">             </label>             <label>               Sticks (10^x):               <input id="sticks-slider" type="range" min="0" max="6" value="3" step="1">             </label>             <label>               Stick length:               <input id="stick-length-slider" type="range" min="0.1" max="1" value="0.4" step="0.1">             </label>             <label>               <button id="generate">Generate</button>             </label>             <label>               Find match to 4 decimals (3.1415):               <button id="find">Start</button>             </label>           </div>         </div>         <p>           If you want to check out a better version of this simulation with           more details on the math, check out           <a href="https://ogden.eu/pi/">Thomas Ogden's website</a>.         </p>  
      ]]></description>
    </item>
  

    <item>
      <title>the crucible</title>
      <link>http://ephjos.io/posts/2022/01/31/index</link>
      <guid>http://ephjos.io/posts/2022/01/31/index</guid>
      <pubDate>Mon, 31 Jan 2022 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>           During a visit to Salem Massachusetts last year, I stumbled onto           Arthur Miller's The Crucible in a bookshop. I figured           there wouldn't be a better place to grab a copy, so I bought it and           forgot about it.         </p>         <p>           All in all, I enjoyed the story. It was quick read and I           enjoyed that most of the characters were ripped straight from           actual events that occurred during the witch trials. Knowing about           Giles Corey and his pressing makes the impact of it in the story           much more intense. Remembering the spot where it occurred, how the           townspeople would have gathered to watch him be slowly crushed.           Recognizing a character's name from his real gravestone was an           impactful experience, and one I hope I don't repeat.         </p>         <p>           The frantic           dialog and the general absurdity made the play chaotic and           engaging, and definitely made its message clear. As an allegory for           McCarthyism, Miller did a great job of showing the lengths           people will go to when they believe they are morally correct.           The pervasiveness of this behavior pattern across time and           place is alarming, and fits in to many situations we face in           modern life.         </p>         <p>           To those in the midst of this mania, everything seems clear: the           Devil has fallen upon Salem. Only           with time does the chaos and confusion of the past become clear.         </p>  
      ]]></description>
    </item>
  

    <item>
      <title>habits</title>
      <link>http://ephjos.io/posts/2022/01/28/index</link>
      <guid>http://ephjos.io/posts/2022/01/28/index</guid>
      <pubDate>Fri, 28 Jan 2022 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>           Another year, another holiday season that I allowed to completely           wreck my schedule and good habits. Right around mid-December, I           lost all discipline. I started snacking and eating more sweets,           sleeping in more, playing video games, stopped doing Advent of Code           and other side projects, stopped writing, stopped reading, and           started spending more time on my phone. I need all of this           to change, so here are my immediate goals for the next month.         </p>          <ul>           <li>Finish Advent of Code (and related post)</li>           <li>Finish the Ray Tracer Challenge in Python</li>           <li>Start the Ray Tracer Challenge in C</li>           <li>Finish reading current book</li>           <li>Start and finish next book</li>           <li>One micro post</li>           <li>Consistent blog posts (1-2 per week)</li>         </ul>          <p>In order to reach these, I need to make some behavioral changes.</p>          <ul>           <li>Read every day before bed</li>           <li>Work on projects every day</li>           <li>Write every day after working on projects</li>           <li>Work out every morning</li>           <li>Leave phone in a designated spot when not actively using it</li>         </ul>  
      ]]></description>
    </item>
  

    <item>
      <title>leverage based todo list</title>
      <link>http://ephjos.io/posts/2021/12/04/index</link>
      <guid>http://ephjos.io/posts/2021/12/04/index</guid>
      <pubDate>Sat, 04 Dec 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         Well its been a month since I started using this todo list every day         (as mentioned in <a href="/posts/2021/11/04">this post</a>), and I         am still sticking with it.          <p>         I found this idea while reading         <a href="http://www.effectiveengineer.com/">           The Effective Engineer by Edmond Lau.</a> This was a great book         overall, and think that it provides a lot of valuable information         about software development in an easy to digest format. It has a         definite Silicon Valley slant, which is acknowledged by the author, but         may put off some readers/make some of the advice not as applicable in         certain situations. For me, the biggest takeaway is the core idea         of using leverage for decision making.          <p>         Leverage for any given task is the         <i><b>value produced per unit time</b></i>.         This provides a concrete way to analyze what tasks should be focused on,         given that one defines their "value" well. A key dimension to consider         is time, be it delayed or repeated returns. It is obvious we should         work on important and urgent tasks (Customer A needs feature X by the         end of the week). However, just as (if not more) vital are         important but non-urgent tasks. These are critical activities         that do not find their way into the main plan, but can spell         disaster if unaddressed. For most teams, these are problems like         scalability, code quality, and hiring. All of these do not provide         much immediate value, but shine when emphasized ahead of time. They         can prevent failures and enable new growth and activities for entire         teams and companies.         Ignoring         them can lead to an entire project grinding to a halt to do a lack of         talent, unmaintable code, or the inability to support a growing user         base.          <p>         Leverage can be applied at the individual level when using todo lists.         Instead of keeping         a sorted list of tasks, have 1 main task and a backlog of tasks.         Periodically perform a pairwise comparison between the main task         and every task in the backlog. If there is a task that provides higher         leverage and covers the switching costs, then consider switching to it.         Otherwise, work on the main task until is complete, then select the         next highest leverage task and move on.          <p>         I have tried todo lists before, but often found myself unable to         stick with it as I'd spend more time curating my list than working on         tasks. This method provides me a low mental overhead while also         giving me enough structure to complete my tasks. I've actually stuck to         this method, and plan to stick with it for the foreseeable future.         All personal and work tasks get thrown on the list, and get crossed         off when their done.          <p>         Software-wise, I've tried to keep it stupid simple and stay away from         more robust (agile-style) applications. I've done todo lists on paper         before, but found myself frustrated when I had an idea or remembered         something but didn't have my notebook handy. I've used apps that distract         with their complexity or didn't provide the same experience on desktop         and mobile. The tool I've been sticking with this time is         <a href="https://teuxdeux.com">TeuxDeux.</a>          <p>         Intentionally restricted         and minimalist, this app provides all of the functionality I need         and nothing more. This flows perfectly with the leverage based todo list         described above, and allow me to separate but keep track of current         tasks and backlog tasks. "Someday lists" can be used for ideas, tasks         that will need to be done in the future, or short lived lists like         grocery and shopping lists. The only other feature I need is the         ability to schedule recurring tasks, which is also built in. There is         not much else to distract me from focusing on crossing off tasks,         which is my main goal here.          <p>         Overall, I think the approach to todo lists outlined by Edmond Lau         is exceptional, especially when combined with the right tool.         I'd recommend anyone who works in or around software engineering         to check out the Effective Engine, and hope you find it as useful         as I have.   
      ]]></description>
    </item>
  

    <item>
      <title>Advent of Code 2021</title>
      <link>http://ephjos.io/posts/2021/12/01/index</link>
      <guid>http://ephjos.io/posts/2021/12/01/index</guid>
      <pubDate>Wed, 01 Dec 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>           Its about that time of year,           <a href="https://adventofcode.com/">Advent of Code is back</a>!           I'll be updating this post with my           thoughts and any visualizations           throughout the challenge. The full source for my solutions           can be found           <a href="https://github.com/ephjos/aoc/tree/master/y2021">in this git repo</a>.           I'm taking this year on using Rust, but I may have to switch to           Python later if Rust is slowing me down too much.         </p>          <div>           <h3>Day 01</h3>           <p>             Today was a good start! The problem was fairly standard and simple,             expected for the first day. Turns out I remember almost nothing             about how to use the Rust standard library and I had to lookup             how to get the length of a vector (its <code>.len()</code>).           </p>           <h4>Part 1</h4>           <p>             Iterate over the list of provided numbers, keeping track of             the previous (initialized to the MAX value), and increment a counter             every time the current number is greater than the previous.           </p>           <h4>Part 2</h4>           <p>             Convert the input lines into a vector, converting to integers along             the way. From 2 to the length of the vector, iterate while             keeping track of the previous sum, and increment a counter             every time             the current window <code>v[i-2] + v[i-1] + v[i]</code> is             greater than the previous.           </p>         </div>          <div>           <h3>Day 02</h3>           <p>             This was another standard AOC problem, I think every year has             several "grid movement from commands" types of problems.           </p>           <h4>Part 1</h4>           <p>             Keep track of x position and depth.             For each line of the input, split out the command and its value.             Based on the command, increment/decrement the correct values.             Multiply x and depth at the end.           </p>           <h4>Part 2</h4>           <p>             Same as above but now up and down modify a new variable instead             of depth, aim.             Now, when moving forward, we update depth using aim and the             current value. Multiply x and depth to get the answer.           </p>         </div>          <div>           <h3>Day 03</h3>           <p>             Today was a grind. I could not shake the feeling that I was             missing an obvious bit manipulation trick, but I eventually             went on to using strings and vectors.           </p>           <h4>Part 1</h4>           <p>             Loop over all of the numbers, keeping count of how which digit             occurred more in each column. Go over the counts, and construct             the bits back into two integers before multiplying them and             returning the result.           </p>           <h4>Part 2</h4>           <p>             The strategy here was iteratively remove numbers from the list             until one was remaining. Each iteration would calculate the             digit counts and then remove all rows that did not match the             key so far. Do this for the first and second rule before             multiplying them and returning.           </p>         </div>          <div>           <h3>Day 04</h3>           <p>             Today was a mess. I am not experienced enough with parsing in Rust,             and today's puzzle format amplified that. I also made the mistake             of trying to mutate a struct/vec in a loop while using its iterator.             This can't be done since it would require 2 mutable references             to be alive at the same time. The easy solution is to fallback             to a simple for loop that iterates an integer over the length of             the array, then getting a mutable reference for each element on             every iteration.           </p>           <h4>Part 1</h4>           <p>             Parse out the draws and boards. Created a board struct with 2             methods. Iterate over the list of draws. For each draw, iterate over             all boards. For each board, mark off (set to -1) the draw then             check for Bingo. If there is a bingo, get the sum of all values             that aren't -1 and multiply this sum by the draw number before             returning.           </p>           <h4>Part 2</h4>             Parse out the draws and boards. Created a board struct with 2             methods. Iterate over the list of draws. For each draw, iterate over             all boards. For each board, mark off (set to -1) the draw then             check for Bingo. If there is a bingo and there are other boards             remaining, remove the board from the board list. If there is a             bingo and this is the last board, return its score.           </p>         </div>          <div>           <h3>Day 05</h3>           <p>             Today wasn't bad at all! Parsing the input was straightforward             and the problem itself was fine. I had some hiccups with using             HashMaps in Rust, but didn't have any issues once I refreshed my             knowledge on them.           </p>           <h4>Part 1</h4>           <p>             Parse out each line into a start and end point. For each pair,             skip if they are not on the same line. Then, iterate over the line             and add each point to a <code>HashMap<Point2d, usize></code> to             count how many times we see the point. At the end, return             the number of values in the hashmap that have a count greater             than 1.           </p>           <h4>Part 2</h4>           <p>             Same as above except for non-linear points, connect             them with a line of slope 1 or -1, which can be determined by             looking at their difference.           </p>         </div>          <div>           <h3>Day 06</h3>           <p>             This puzzle was one of my favorite types of AOC problems, where             part2 is just part1 but scaled up. This usually involves some kind             of exponential growth and greatly punishes the naive approach.             I suspect there is a straight numerical solution (this whole thing             screams rings), but I'll search for that later since my solution is             more than fast enough.           </p>           <h4>Part 1</h4>           <p>             I back-ported my solution for part 2 here, but I initially             implemented the naive approach. Parse the list of input numbers             into a vector. For 80 iterations, look at each element in the fish             age list. If the current age is 0, set it back to 6 and add a fish             to the end with age 8. Otherwise, decrement the current age.             Return the length of this array.           </p>           <h4>Part 2</h4>           <p>             Instead of keep a list of fish directly, I instead chose to keep             two lists, fish and new_fish. These vectors have length 7 and 9             respectively with the value representing the number of fish             at the age represented by the index. So if <code>fish[2] = 12</code>             that means there are 12 2-day old fish. Using this, I can             simply move over these vectors and "slide down" the groups of fish.             I keep 2 separate lists due to the different age resets: a new             fish starts at 8 and an old fish starts at 6. We perform the             same shuffle and add between these lists in each loop before             returning the sum of their sums.           </p>         </div>          <div>           <h3>Day 07</h3>           <p>             These problems were fairly straightforward, but I messed myself up             on some integer rounding. I picked up on some statistical values             like median and mean being relevant, but didn't go with them on             the first try. I ended up brute forcing both parts and getting             my solutions before going back and optimizing my solution             based on others posted online.           </p>           <h4>Part 1</h4>           <p>             For every possible position, calculate the total cost and see if             it is less than the minimum, saving if it is. Or, find the             median and calculate the cost to it.           </p>           <h4>Part 2</h4>           <p>             Same as above, except the cost function is the sum from 0 to x.             Or, recognize this as the gaussian function <code>n*(n+1)/2</code>             and that the solution will be at either integer directly above             or below the mean.           </p>         </div>          <div>           <h3>Day 08</h3>           <p>             This day was hard, and I actually had to finish it the next day             which is never enjoyable. The first part is trivial, but the             second is not so much. I was able to come up with a solution,             but it was a grueling experience.           </p>           <h4>Part 1</h4>           <p>             Parse the input into "uniques" and "outputs". Count the number             of output strings with lengths 1,2,4, and 7.           </p>           <h4>Part 2</h4>           <p>             The approach I used is far from the most efficient but was the             only way I was able to logically step through the problem. Given             the easy to find numbers from above, compare them and start to             uniquely identify segments. The top is the segment present in 7 but             not in 1. The middle is the segment present in 4 and all of the             5 length strings. Carry this on, comparing what segments using             their size and previous segments until a full mapping is defined.             Iterate over the outputs and convert them to digits using this             mapping, then add the number they make to counter. Return the             counter.           </p>         </div>          <div>           <h3>Day 09</h3>           <p>             These problems were much better than yesterday's.           </p>           <h4>Part 1</h4>           <p>             Parse the input into a grid. For each number in the grid, check             if it is smaller than all of its neighbors. If so, it is a minimum.             Track the indices of these minpoints. Return the amount of             minpoints found.           </p>           <h4>Part 2</h4>           <p>             For each minpoint, perform a breadth first search where 9s are             not considered neighbors and return the size of the explored set.             Collect these sizes into a (max) binary heap, then pull off and multiply             the 3 biggest, returning their product.           </p>         </div>          <div>           <h3>Day 10</h3>           <p>             I was unable to get to AOC on the day this problem came out, so I             did it on Day 11, along with that day's problem. I enjoyed these             two problems, and found that they were pretty easy to solve.             With all of these bracket matching type questions, a stack is             your best friend.           </p>           <h4>Part 1</h4>           <p>             Initialize a stack, the provided point map, and a points counter.             For each line, iterate over its characters. If one of the opening             brackets is found, push its corresponding closing bracket onto the             stack. Otherwise, pop the top of the stack and check if the             current charcter matches the one on the top of the stack. If it             doesn't,  perform the point lookup and update the counter.             At the end, return the total count.           </p>           <h4>Part 2</h4>           <p>             Start with all of the above, except perform the counts per line.             If the point total for a line is not 0, it was corrupted and we             can skip it. Otherwise, the autocomplete_score will be the score             calculated using the characters remaining on the stack.             Create an autocomplete_score_map, and use that with the provided             algorithm to calculate the autocomplete_score. Return the             autocomplete_score over all lines.           </p>         </div>          <div>           <h3>Day 11</h3>           <p>             This day was straightforward but enjoyable. The gotcha with these             automata-type problems can be nasty, but this specific setup             wasn't too bad.           </p>           <h4>Part 1</h4>           <p>             Read the input into a grid. Create a step function that modifies             the grid and returns the number of flashes. This function first             increments everything in the grid by 1. Initialize the flash counter             and a boolean to keep track of if a flash happened             (default to true). While a flash has happened, set the flash boolean             to false and loop over all of the             cells and check for one > 9. If so, set it to 0 and increment all             of its neighbors by 1 that are also not 0. Set the flash boolean             to true and increment the flash counter by one. Return the flash             count. In the main function, create another counter. Call the step             function 100 times, adding each flash count to the total counter.             Return this total count.           </p>           <h4>Part 2</h4>           <p>             Do the same as above, but ignore the flash_count. Loop until             all of the cells in the grid are 0, keeping track of how many             iterations its takes. Return the number of iterations.           </p>         </div>          <div>           <h3>Day 12</h3>           <p>             This was rough. It took me way too long to recognize that I should             use DFS over BFS. In my implementation of DFS, I made the mistake             of having top-level logic along with logic inside of the neighbors             loop. This lead to me missing some edge cases in part 2 and             wasting a lot of time on a simple mistake. Also, given that             we are often revisiting nodes, a visited set is better replaced             with a path vector. Removing from the set can remove items that             are in the middle of the path, which is invalid and leads to             infinite loops and hard to debug stack overflows.           </p>           <h4>Part 1</h4>           <p>             Use the input to build a map from node to list of neighbors.             Starting with the "start" node, perform dfs with an additional             check for revisiting small rooms. Count the number of paths             that lead to "end" and return this sum.           </p>           <h4>Part 2</h4>           <p>             The same as above, except we introduce a boolean to keep track             of whether or not we've used our one small room revisit. It             starts as false, and is set to true when we revisit our first             small room. On subsequent calls, when we revisit another small             room, we return 0 and exit early if we've already used our revisit.             Part 1 is the same as Part 2 where this boolean starts as true.           </p>         </div>          <div>           <h3>Day 13</h3>           <p>             This problem was great! I enjoyed this one a lot and did not             anticipate the direction of part 2. I didn't have any issues             with today, which was much needed after yesterday.           </p>           <h4>Part 1</h4>           <p>             Split the input into two big chunks using "\n\n". The first chunk             lays out all of the dots and the second contains the fold             instructions. Parse out the Points from the dots and             the folds. I used a Point2d struct to represent the (x,y) coords             and a Fold struct that holds the axis and line for each fold.             For the first fold, iterate over all of the dots. If the fold             is on the y axis, and the current dot's y value is greater than             the folding line, reflect the y value over the line. Otherwise,             return the dot unchanged. It the fold is on the x axis, do the             same but with the x values. Reflecting the value is as easy as             <code>line - (v - line)</code>, thanks to the fixed folding             direction. To handle overlapping, I store the dots in a HashSet             instead of a Vec. This allows me to just return the length of             the set for the answer           </p>           <h4>Part 2</h4>           <p>             These are my favorite kinds of solutions. Throw all of the above             logic into a loop over all of the folds. After performing all             of the folds, get the max x and y values from the dots. For each             point on the grid they lay out, print out the empty string if             there is no dot, or a # if there is a dot at that point. This             will print out a grid that will draw out the 8 capital letters             that make up the answer.           </p>         </div>          <div>           <h3>Day 14</h3>           <p>             Today was a bit tricky, but went smoothly once I recognized             how to get the answer for part 2. I figured this would be             one of the problems where the naive approach in part 1 would             not be feasible in part 2, but it took me a few minutes to             figure out a faster solution.           </p>           <h4>Part 1</h4>           <p>             Split out the input into the template and list of rules. Create             a HashMap to keep a count of the characters. For 10 iterations,             do the following. Get the 2 character windows over the current             string. For each window, lookup its new value and insert it.             Increment the count of the new character. After 10 iterations,             get the maximum and minimum counts then return their difference.           </p>           <h4>Part 2</h4>           <p>             Increasing the iterations to 40 here makes actually building             the list unfeasible. Instead, we will keep track of the count             of each pair of characters. Each iteration, save the current             state of the map and loop over all             of the known keys. Since the updates are supposed to happen             at the same time, we can't modify and read from the same             HashMap during the key loop. Get the count from the copied             HashMap and increment the counts for the new pairs by its values,             decrementing the value of the original pair.             If <code>AB -> C</code>, and <code>count[AB] = 3, count[AC] = 0, count[CB] = 0</code>, then             <code>count[AB] = 0, count[AC] = 3, count[CB] = 3</code>.             This builds the count of each pair, but does not account for or             encode their overlap. To remedy this, we also keep track of             the individual             character count. At the end of 40 iterations, we return in the             same fashion as part 1 using these individual counts.           </p>         </div>          <div>           <h3>Day 15</h3>           <p>             Welp, the holidays happened and I didn't start this problem until             early January. Looks like I'll be playing catch-up yet again.             This problem was fairly straightforward, it is just shortest path             with a larger graph in the second part. This is most easily             accomplished with Dijkstra's algorithm. I had to actually use the             min-heap optimization for part 1, which the Rust docs             <a href="https://doc.rust-lang.org/std/collections/binary_heap/index.html">               contain a great example for</a>. This was fast enough to solve             part 2 as well.           </p>           <h4>Part 1</h4>           <p>             Parse the text into a grid of danger values. Run Dijkstra, return             result.           </p>           <h4>Part 2</h4>           <p>             Follow the rules to create the new 5x5 megagrid. Run Dijkstra,             return result.           </p>         </div>          <div>           <h3>Day 16</h3>           <p>             I am currently trying to build my habits back, which includes             finishing up these problems. This problem was a bit of a pain,             as it took me a bit to get the parsing right and once I did,             I completely forgot that I was defaulting one of my return values             to the empty string. It took an embarrassing amount of time to spot             this, but I eventually got everything working. As is tradition with             AoC, I was rewarded with building a full parser in the first part             by making the solution to part two as easy as adding a method             to my Packet struct.           </p>            <p>             I am going to trim out the specific part comments for the rest of             these as I am trying to breeze through these problems and wrap up             AoC 2021 by March 2022.           </p>         </div>          <div>           <h3>Day 17</h3>           <p>             This day was fun, but challenging up front. I figured there would             be both a brute force method and a mathematical solution, but             neither one was clear. I took some time to think through the             constraints of the problem, and slowly was able to build up             my velocity nested loop bounds one by one. The maximum x speed             is going to be the further x point away in the target area. Anything             faster immediately fails after a single step. Almost identically,             the minimum y speed is             the minimum y value in the target area, since anything faster always             misses regardless of x value. I first ran the program with a minimum             x velocity of 0 and a maximum y velocity of 1000. This was clearly             inefficient but worked, allowing we to get both solutions.           </p>           <p>             I further refined the loop bounds by recognizing that I could             figure out the minimum x speed with some pre-computation: this             speed is largest one whose Gaussian sum that is less than the             minimum x value in the target. Every speed less than this             falls short. Then, due to conservation of momentum I realized that             the maximum y velocity is the absolute value of the minimum y value             in the range. This works since all targets (that I have seen)             are below y=0. After going up, the projectile comes back down and             passes y=0 with its starting velocity but negated. And velocity             greater than the furthest target point away immediately misses on             the first tick.           </p>           <p>             While a bit of a grind, I enjoyed these problems.           </p>         </div>          <div>           <h3>Day 18</h3>           <p>             This day took me way too long. From my Haskell experience, I             immediately saw that a recursive data structure could represent             the problem nicely, but that is usually not a good idea in Rust.             I eventually realized that keeping track of values and depths             would probably be enough to get it done. Throw in a busy week,             some misreading of the problem, and few off-by-one errors and             this problem goes from fun to painful in a flash. After solving,             I looked at some other solutions and saw that my Haskell intuition             was correct and lead to elegant answers. I also saw that this may             have been a great situation for a struct-of-arrays instead of             my usual array-of-structs approach. I'll try to keep this in mind             going forward.           </p>         </div>          <div>           <h3>Day 19</h3>           <p>             This problem stunk. I struggled understanding the question for a             bit, and it took me even longer to realize that subtraction can             be used to find when two sets of points are similar but offset             from each other. I then spent a few hours trying to understand             why my solution wasn't working only to realize that I simplify             forgot to add an offset back in to a calculation. I did not enjoy             this problem, and it makes me want to finish up and             put AOC2021 behind me.           </p>         </div>          <div>           <h3>Day 20</h3>           <p>             This problem wasn't too bad, and I actually found myself enjoying             an AoC puzzle again. All of the fun stuff was here: parsing,             2D grid data, binary number creation, cellular automata, and a final             solution that contains a cool visual.             I got hung up by the weird infinite grid aspect, but sorted it             out with an additional boolean. I predicted that part 2 would             just be a scaling up of part 1, and I was correct. My solution             averages around 1100ms to solve both parts, but I would like to             speed it up more. If every day was like this, I would be enjoying             this process much more.           </p>         </div>          <div>           <h3>Day 21</h3>           <p>             This problem was enjoyable. I expected to be thrown off with             some ring math again due to the clear modulo in the problem             structure, but the problem did not go that way. Part 1 was             easy enough, once I sorted out how to do modulo [1,n] (subtract             1 before the modulo and add 1 after). Part 2 was daunting at             first, and the shear scale of possible games made recursion             seem unfeasible. I looked to see if the Chinese Remainder             Theorem could apply, but nothing of the sort popped out. I             then realized that we only care about the result of the 3 rolls,             not each roll individually. Since take the sum of these 3 rolls,             we can greatly reduce the search space thanks to the repeated sums.             There are 27 permutations with repeats for 1,2, and 3. However,             there are only 7 unique sums from these 27. Instead of             recursing for each of the 27 permutations, we can recurse for             each of the 7 sums and multiply the result by the frequency             of that sum. This worked first try, and I was proud to see             this approach on my own. I'm sure there is a faster way to do             this, and likely strip out some other branches too but I am             not interested in that right now. I am hoping for more             problems like this in the last 4.           </p>         </div>          <div>           <h3>Day 22</h3>           <p>             Part 1 was great! Create some sets, do some adding and removing, then             count up the result. With part 2, this won't work. I realized that             I needed some way to either break apart cubes when intersected, or             manage the intersections correctly so that I could add their volume             for the answer at the end. I struggled with this for so long, trying             both approaches. I could not get either one to work, so I had to cave             and checkout someone's solution on Reddit to see what I was doing wrong.             I was being a little naive about how I was building my list of intersections,             and also missed a trick with the on value to correctly cancel out             intersections. I had the majority of the code written, but had things             a bit out of order. I did not enjoy this problem.           </p>         </div>          <div>           <h3>Day 23</h3>           <p>             This day wasn't too bad. I was sure that I knew how to get the answer             from the start using BFS, but I wasn't sure if this would work for the             second part. I went ahead anyway, and luckily the second part did not             increase the search space to the point that BFS was not feasible.             The complexity here could be improved quite a bit using Dijkstra's or             getting fancy with A*, but I'd rather finish the other problems first.           </p>         </div>          <div>           <h3>Day 24</h3>           <p>             <b>This day was horrible.</b> This was easily the most difficult             problem I have seen in AoC and could not get the solution on my             own. I had to look at hints and solutions on the subreddit in order             to finally solve this. I appreciate the spirit of these reverse             engineering style questions, but picking out the pattern in this             one was next to impossible for me. I used the method from             <a href="https://github.com/dphilipson/advent-of-code-2021/blob/master/src/days/day24.rs">               this solution,</a> and was able to actually implement it in code             instead of doing it by hand. So while I completely failed this             problem, I was still able to implement a general purpose solution             which I understand has been difficult for many other people.             At the time of writing (03/13/2022 oops), only ~12,000 people have             completed both parts of this problem.           </p>           <p>             I am not against this style of problem, if anything my inability to             solve it means I should be exposed to more problems like it.             If there was a way to make a hint towards the solution and/or             provide test data without spoiling the solution completely, it             may have helped with the completion rate of this problem.             I did not enjoy this experience, but at least I am now aware of a             style of problem that I struggle with.           </p>         </div>          <div>           <h3>Day 25</h3>           <p>             I enjoyed this problem. A nice, straightforward cellular automata             iteration problem. My solution could be so much better, but it             worked first try. I was aware of the tradition that the last star             is given for free, so I didn't worry about having to adapt             my solution for part 2.           </p>           <p>             This whole process did not go the way I wanted it to. I got busy             with the holidays and completely lost discipline to work on this             the way I should have; but I am happy to finally complete it.             If I prepared ahead of time by doing old puzzles, I could have             solved some of the early problems faster. This would have made             it easier to fit into my schedule and the whole effort easier             to stick to. If I try to do this again this year, I will definitely             prepare in November. Although, I may just save the problems to             do throughout the year and spend December on something else.           </p>           <p>Merry Christmas!</p>         </div>  
      ]]></description>
    </item>
  

    <item>
      <title>juvenoia</title>
      <link>http://ephjos.io/posts/2021/11/28/index</link>
      <guid>http://ephjos.io/posts/2021/11/28/index</guid>
      <pubDate>Sun, 28 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         I read an excellent post today from Morgan Housel called         <a href="https://www.collaborativefund.com/blog/experts/">           Experts From A World That No Longer Exists</a>. The core idea         here is that the work required to become an expert in a field         also builds up resistance to certain ideas. Experts are often         against trying ideas that have failed in the past and are quick         to caution new-comers about these ideas. Inversely, new-comers         do not have the established knowledge and experience which may         limit their overall ability but leaves their thoughts completely         unobstructed. The idea was driven home for me with the examples         given from the dot-com boom; business ideas that were shunned         and failed have since become multi-billion dollar businesses         like Chewy and Amazon. One must allow their skills to develop         to an expert level while still seeing the world as a newbie, or         watch the world that they are an expert in cease to exist.         Those of us who refuse to try something since it has already         been tried before will miss out on opportunities that arise         as our world changes around us.          <p>         One idea that is closely tied in with this article is that         of         <a href="https://en.wiktionary.org/wiki/juvenoia">           Juvenoia</a>, a word used to describe an older generation's         irrational fear of the younger one. In this context, this         pertains to the expert's fear and misunderstanding of the         new-comers. Just because you can't understand them and/or         they are different does not inherently mean that their         actions or ideas are worthless. If anything, some of the         most radical changes of the past century have come from a new         generation taking something that "belonged" to their         forefathers and making it their own. Rock and roll, hip hop,         and first person shooters are all things born out of putting         a spin on something that existed before. At the time,         the old generation recoiled at these changes, saying that the         "kids these days are doomed".          <p>         Don't get stuck screaming about your lawn; develop the skills of an         expert but maintain a childish view of the world. Only a fool         would think anything is possible.  
      ]]></description>
    </item>
  

    <item>
      <title>pypy is cool</title>
      <link>http://ephjos.io/posts/2021/11/25/index</link>
      <guid>http://ephjos.io/posts/2021/11/25/index</guid>
      <pubDate>Thu, 25 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         In the last post I talked about the performance of the ray tracer         I am writing in python and found myself quite unhappy. I did some         digging into alternate interpreters and found         <a href="https://www.pypy.org/">           pypy</a> which does JIT compilation and focuses on long running         computationally intense tasks. It sounded up for the job, so I installed         it, got the right pip version and installed some packages, then ran it.          <p>         The cover image from the last post took ~17 minutes to render at 300px         by 300px. With pypy, the same image is rendered in ~4 minutes. In about         ~20 minutes, I was able to render it in 800px by 800px. This is all         a substantial improvement, and I am happy that it'll help speed up         my work until I get to more optimizations. I am also able to push         the rewrite in a compiled language into the future as I don't         think it'll be required for me to finish the book and render some         images.          <hr />          <p>         Today is Thanksgiving, and I am deeply thankful for all of the people         and things that make life worth living. I feel grateful for life         itself and everything that comes with it, the good and the bad. Being         alive is a great thing.          <p>         Happy Thanksgiving!  
      ]]></description>
    </item>
  

    <item>
      <title>pretty pictures, slowly</title>
      <link>http://ephjos.io/posts/2021/11/23/index</link>
      <guid>http://ephjos.io/posts/2021/11/23/index</guid>
      <pubDate>Tue, 23 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         I've hit a milestone with my implementation of the Ray Tracer         Challenge, I can render the cover image of the book!         <img           alt="cover image"           src="/img/rtc/cover.webp"           width="300"           height="300">          <p>         This was my primary goal when I first started this book, so I am pretty         stoked to have made it here. However, I am having major performance         issues. The above image took 17 minutes to render, which is not         great. It is significantly slowed down by using a ray bounce depth of         5, so every ray that hits a reflective surface has the potential         to 5x the computation, which adds up. I've applied various         optimizations already like caching inverse matrices, reducing         allocations by mutation, and rewriting the core tuple and matrix         code to be more efficient. These all provided a significant speed up,         but it is not quite enough. I've experimented with multithreading,         and saw a ~2x speed up, but I want to keep this single threaded.           <p>         Here's what the profiler output while generating the above image         (using snakeviz for visualization).          <img           alt="python profile graph screenshot"           src="/img/rtc/prof1.webp"           width="1385"           height="778">          <p>         As we can see, there is nothing unexpected. We spend the vast         majority of the time multiplying matrices together. I've focused         on this code, and can't squeeze much more performance out of it.         I experimented a bit with numpy and numba here, but didn't see much         improvement (and don't want to use them in this project). The problem         seems to be that we are wasting a lot of work. I've read ahead about         using a Bounded Volume Hierarchy to only check for intersections         on objects within a bounding box, and I look forward to the         potential speedup. As it is now, the above image required         172,535,400 calls to the <code>matrix_mul_tuple</code> function to be rendered in         300px by 300px. The book notes about an order of magnitude reduction in         the amount of intersection calls when using BVH on an appropriate scene,         which I hope to see as well.          <p>         I plan to implement BVH then crunch through the rest of the book         pretty quickly, given that I've done this all before. I'll explore         further optimizations at the end, and then look into doing another         implementation with performance in mind. I've been particularly         motivated by a site I found on the book's forum that         <a href="https://iliathoughts.com/posts/raytracer/">           demos a C++ implementation compiled to WASM</a>. If I can get         on the same order of magnitude of performance, I'll be happy.          <p>         I've been enjoying the process lately, which is great! While this         ray tracer is not the most performant (as admitted by the author), it         does produce         some of the better images that I have seen when compared to other         hobbyist/educational ray tracers. I am happy with the image quality         now, but I want to focus on performance before doing more post-book         additions like depth-of-field and anti-aliasing. For now, here's         two scenes from the recent chapters:          <img           alt="reflections and refractions image"           src="/img/rtc/reflect-refract.webp"           width="800"           height="800">         <img           alt="table cube example image"           src="/img/rtc/table.webp"           width="800"           height="800">  
      ]]></description>
    </item>
  

    <item>
      <title>lunar eclipse</title>
      <link>http://ephjos.io/posts/2021/11/19/index</link>
      <guid>http://ephjos.io/posts/2021/11/19/index</guid>
      <pubDate>Fri, 19 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
               <p>         As I was standing in my backyard this morning at 4am, watching the         lunar eclipse, I realized that its been a long time since I stared         into the night sky. Even in a light-polluted residential neighborhood,         the amount of stars you can see once your eyes adjust is incredible.         Its been a while since I've felt that weird feeling of cosmic scale.         I felt awe when it hit me that I was standing in the same shadow         as the moon. It all looks so far away yet so close.          <p>         The most incredible thing I noticed is that stars twinkle.          <p>         <i>I forgot that stars twinkle</i>          <p>         I've been so disconnected from the night sky that this just wasn't in         my head. I found the little dipper and started looking around         for other constellations when this hit me over the head. The         immensity coupled with understanding that the Earth         is the most special place in the known universe always make         me feel insignificant         but unbelievably lucky; lucky to be alive on this         <a href="https://en.wikipedia.org/wiki/Pale_Blue_Dot">           Pale Blue Dot</a>.          <p>         I need to stargaze more.   
      ]]></description>
    </item>
  

    <item>
      <title>the wrong abstraction</title>
      <link>http://ephjos.io/posts/2021/11/18/index</link>
      <guid>http://ephjos.io/posts/2021/11/18/index</guid>
      <pubDate>Thu, 18 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         Sandi Metz has a great short article called         <a href="https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction">           The Wrong Abstraction</a>. As software developers, we spend a lot         of our time and effort on abstractions. All of us understand what         it is like to run up against a bad abstraction that just doesn't         seem to fit the problem. The interesting angle that Sandi takes         is to <i>prefer duplication over the wrong abstraction</i>, due         to the disproportionate cost of bad abstractions. This is an         interesting idea that makes for an approach that only adds         abstractions once they are needed and obvious, or when the code         becomes <i>too</i> repetitive. To untangle an existing abstraction,         simply locate the places the abstraction is used and copy/paste         the abstraction inline. This provides the benefit of immediately         removing the dependency while providing the ability to more readily         identify and define a better abstraction.          <p>         All in all, this post is based on a simple idea, but it is a slight         shift in the way to approach problems that most of us are taught.         Instead of focusing on the existing abstraction and fitting the         problem to it, focus on removing bad/unneeded abstractions and only         add them where they truly provide value.  
      ]]></description>
    </item>
  

    <item>
      <title>parkinson's law</title>
      <link>http://ephjos.io/posts/2021/11/16/index</link>
      <guid>http://ephjos.io/posts/2021/11/16/index</guid>
      <pubDate>Tue, 16 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <blockquote>           work expands so as to fill the time available for its completion         </blockquote>          <p>         I stumbled onto the Wikipedia page for         <a href="https://en.wikipedia.org/wiki/Parkinson%27s_law">           Parkinson's law</a> today, and was intrigued. I had never heard this         name used for describing this specific phenomenon.         This aligns with the idea of scope creep which often comes up in         software engineering. We often view extra resources as an opportunity         to do more, which can be extremely detrimental to the goal of the         project.          <p>         This concept was referenced regarding this great post about         <a href="https://mzrn.sh/2021/11/14/how-i-helped-build-a-profitable-mvp-over-a-weekend"> building a profitable MVP in a weekend</a>.         This MVP was successful due to the owner being well connected and         experienced in the space, but also due to an aggressively small         scope. Instead of focusing on what is possible in a weekend and         listing out features, the author instead focused on whittling         down the app to the smallest possible size. This narrow focus allowed         them to ship in a weekend and start making money.          <p>         This is something that I am trying to apply to my projects, both         before and during them. I have become much more comfortable with         erasing ideas from my project list before starting them. Once started,         I actively work to <i>reduce</i> the amount of work remaining. This         leads to me focusing on more important tasks while working on and         finishing them more consistently.          <p>         The space of projects I <i>could</i>         do is infinite, but my time is not. Being able to pick out the most         valuable and fun projects from that space will give me the highest         yield for my effort. Once started, instead of drumming up new features         to add, it is almost always a better idea to start something new.  
      ]]></description>
    </item>
  

    <item>
      <title>docker development container for node</title>
      <link>http://ephjos.io/posts/2021/11/12/index</link>
      <guid>http://ephjos.io/posts/2021/11/12/index</guid>
      <pubDate>Fri, 12 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         Using a docker container for node.js development is a good idea for         a few reasons. It easily allows the development environment to be         shared with others and provides <i>some</i> amount of separation         from the host system. While far from being locked-down, running         <code>npm install</code> inside of the container can help prevent         install scripts from exploiting the host machine. Given how common         <a href="https://github.com/advisories?query=severity%3Acritical">           these types of vulnerabilities</a> have become, this is something         that any npm user should be aware of.          <p>         The setup is simple, consisting of a <code>Dockerfile</code>,         <code>docker-compose.yml</code>, and a single <code>package.json</code>         script. This will let you spin up the container and get a shell inside         of it to install packages and run commands. It can be further modified         to automatically run a command and be used for deployment, but I'll         leave that as an exercise for the reader.          <h3>Dockerfile</h3>         <pre><code>FROM node:latest WORKDIR /app/ COPY package.json . RUN npm install COPY . .</pre></code>          <h3>docker-compose.yml</h3>         <pre><code>version: '3.8' services:   project:     build:       context: .     command: bash     volumes:       - .:/app/       - /app/node_modules</pre></code>          <h3>package.json</h3>         <p>         You can use an existing package.json, or create one for yourself by         running <code>npm init</code>.         <pre><code>...   "scripts": {     ...     "docker:shell": "docker-compose run project",     ...   }, ...</pre></code>          <p>         Once that's setup, run <code>docker-compose up --build -d</code> to         start the container and then <code>npm run docker:shell</code>         to get a shell inside of it. From their <code>npm install</code>         and everything is up and running! Get out of the container by running         <code>exit</code> inside of it. Use         <code>docker-compose down -v</code> outside of the container         to completely tear it down if you want a clean start.          <p>         If you plan to run a server, you need to be sure to expose and use         the proper ports. Thanks to the container being mounted in the project         directory, you can use your favorite editor to create and modify files         on your host system while running them on the container.          <p>         This is far from perfect, but is a step in the right direction         and should provide some value to the developer and <i>maybe</i>         some small amount of security.  
      ]]></description>
    </item>
  

    <item>
      <title>distractions</title>
      <link>http://ephjos.io/posts/2021/11/11/index</link>
      <guid>http://ephjos.io/posts/2021/11/11/index</guid>
      <pubDate>Thu, 11 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         Lately, I've been going through a weird distraction cycle with this         website. Every few days, I'll convince myself that I have to         make some massive change to the project's structure, introduce         new technologies, and further complicate things. Down the rabbit         hole I go for a couple hours before clarity bonks me over the head.         "I don't need this" I think to myself as I realize the time I just         wasted.          <p>         As stated before, I get distracted by new tools and features incredibly         easily. By intentionally keeping the scope of this site small and         boring overall, I give myself more time to actually fill it with         content. I am hoping that I can shorten these distraction cycles         further, hopefully to the point where they stop happening. Here's some         reminders for myself.          <ul>           <li>This site does not need markdown</li>           <li>The most you have ever written is with this current setup</li>           <li>The deploy process does not need new tools brought into it</li>           <li>The goal here is to write and stay minimal, not to tinker with new toys</li>           <li>If I want to try something out, I can do it separate from this site</li>         </ul>        <p>Here are some changes that I will consider for the future:        <ul>         <li>           More efficient builds (don't rebuild index pages and RSS feeds from           scratch every time)         </li>         <li>           Scope index pages to N most recent posts, create separate archive           page with all of the posts to keep indexes small         </li>         <li>           Develop a simple tool to do SSI at build time, reduce the work of           the server.         </li>       </ul>        <p>       In reality, these are all fairly low leverage activities but they all       provide some value towards efficiency and keeping things clean. For now,       none of that is needed and I'd rather spend my effort on writing and       other projects.  
      ]]></description>
    </item>
  

    <item>
      <title>reorganizing into python modules</title>
      <link>http://ephjos.io/posts/2021/11/09/index</link>
      <guid>http://ephjos.io/posts/2021/11/09/index</guid>
      <pubDate>Tue, 09 Nov 2021 12:00:00 -0500</pubDate>
      <description><![CDATA[
              <p>         I take back what I said the other day about reorganizing my         python code for the Ray Tracer Challenge; I moved the code into         modules. Its been a while since I've done this, so there were plenty         of things I got wrong, but I am happy with it now. I broke the         code into three directories: rtc, test, demo. This allowed me to         break tests and demos out into separate files, which prevents         the circular import issues. The tests can be run using         the unittest module's discover functionality. The demos         are run by running the module directly which calls the         <code>demo/__main__.py</code> file.          <p>         This was a small reorganization and I like the result. In the past,         I usually just filled directories with python files, but I can         how this would have been useful for some larger projects.         I'll approach this like optimization: only do it when it becomes         necessary. I am reminded of the famous quote:          <blockquote>           Premature optimization is the root of all evil           <p>             - Donald Knuth           </p>         </blockquote>          <p>         I'll use modules once the individual files become a mess,         but stick to the files where I can.          <hr />          <p>         As of now, I am going to try posting here 3 days a week: Tuesday,         Thursday, and Saturday (or Sunday). I also want to write longer posts         on the main blog, but don't know how often I can get to that. I am         still going to try to write every day but not force myself to post         rushed writing.  
      ]]></description>
    </item>
  

    <item>
      <title>30</title>
      <link>http://ephjos.io/posts/2021/11/05/index</link>
      <guid>http://ephjos.io/posts/2021/11/05/index</guid>
      <pubDate>Fri, 05 Nov 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         My python ray tracer for the Ray Tracer Challenge has just about         caught up to where I was two years ago with my Haskell implementation.         Look, I'm back to phong shading!          <img           src="/img/demo_material.webp"           alt="phong shaded sphere"           width="480"           height="480">          <p>         I am feeling much more confident and I am actually enjoying working         through this book again. Now that this project is framed in a much         better way in my mind, I find myself looking forward to working         on it. This is a long way from the dread I felt the few times I         tried to pick it up again last year.          <p>         I also did a little cleanup of the code which was enjoyable, removing         unneeded static methods. The whole import tree is still needlessly         complex and some files export the demo functions for other functionality         in order to prevent circular imports. These should probably be broken         out into their own files, but it doesn't matter for now          <hr />          <p>         Wow, Day 30! I am honestly surprised that I did this, and even more         surprised that I enjoyed it overall. Some days were a total slog,         which I think is evident from the writing (especially the one where         I had to get myself out of bed at 11:30pm when I realized I forgot         that day's post). Most surprising was the amount of days that         writing these shorts posts seemed dreadful but turned into great         (or at least fun to write) posts.          <p>         From a standstill, the idea of writing often seemed like a lot of         boring work. But, once I got the ball rolling, I noticed I had a         whole lot of ideas that I wanted to write about. This whole effort         has had me reading, thinking, and daydreaming more. I still         want to reduce the amount of content I consume, since this blog has         illuminated just how much noise I have every day.          <p>         I am going to lax my constraint of writing here <i>every</i> day, but         I absolutely want to keep this habit up in some way. Maybe a weekly         goal works best, combined with a monthly goal for the main blog.         Tentatively, I'll commit to 3 micro posts a week and 1 blog post a         month. It'd probably be best to pick specific days and dates, but I'll         save that for later. Maybe I'll bring back the daily writing with         this year's         <a href="https://adventofcode.com/">Advent of Code</a>.          <p>         This has been a great experience and I can definitely say now that I         enjoy writing, at least in some capacity.  
      ]]></description>
    </item>
  

    <item>
      <title>29</title>
      <link>http://ephjos.io/posts/2021/11/04/index</link>
      <guid>http://ephjos.io/posts/2021/11/04/index</guid>
      <pubDate>Thu, 04 Nov 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         Great ideas are everywhere, the execution is what is rare.         Many a self help book has laid out the painfully simple steps to         get in shape or improve in one's career. Read through enough of them         and the overlap of ideas will become clear. It is not about reading         or having as many good ideas as possible, it is about consistently         executing on them.          <p>         I have read, watched, and listened to hours of media about time         management and creating focus. Finding a state of flow can be near         impossible, but it is an incredibly valuable place for a software         developer to be. I'd get inspired reading a book, break out a journal,         and start my daily schedule. Without fail, I would just stop after         a couple days and went right back to operating without a plan.         Like many other obstacles in my life, I noticed that I was feeling         a lot of friction when it came time to plan.          <p>         So, I am taking another         pass at things using a new piece of software that is much more         simple than what I've used before and is easier to work with         (and access) than a paper journal. I'll be taking a simplified         approach, focusing on an unordered list of tasks and the value         of the one in progress. When something on the list obviously         surpasses the current task in value, I'll switch my focus.          <p>         I'll update back here with the results after a month. Here we go again.  
      ]]></description>
    </item>
  

    <item>
      <title>28</title>
      <link>http://ephjos.io/posts/2021/11/03/index</link>
      <guid>http://ephjos.io/posts/2021/11/03/index</guid>
      <pubDate>Wed, 03 Nov 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I had a realization today that I <i>think</i> I may be able to get         my old Haskell ray tracer up and working. As it turns out, I may have         been completely wrong when expecting the program to be able to         generate high-resolution images. The implementation obviously has its         flaws, however I think I was simply expecting too much of it. This         clicked when I realized that the book suggests using much lower         resolutions than I expected when rendering. Even for a completely         blank canvas, the design of the program will lead to high memory usage.         This usage quickly exceeds what is feasible, aside from other         implementation details.          <p>         I am going to max out my python implementation at 640x480 images and         try to finish the book. If I find myself inspired, I'll loop         back to my Haskell implementation (still hidden in my git history)         and try the same there.          <p>         I don't know why this never hit me before, but I'll chalk it up to         the Covid outbreak starting while I worked on this. That sounds like         a good enough excuse to me :)          <p>         The affect that perspective and time spent away from a problem         can have on one's perception of the problem will always surprise me.         This seems dead simple to me now.  
      ]]></description>
    </item>
  

    <item>
      <title>27</title>
      <link>http://ephjos.io/posts/2021/11/02/index</link>
      <guid>http://ephjos.io/posts/2021/11/02/index</guid>
      <pubDate>Tue, 02 Nov 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         python is almost always a pleasure to use. I find that is helps         drastically lower the barrier between my brain and the computer,         letting ideas quickly become code without much friction. While this         may not be great for maintainability long-term, it is         great for "just doing" something which is often my goal. I like         using type systems and compiled languages, but I think python is         a better choice in most cases as it lets me focus on the problem at         hand without much effort. I should use python more often, especially         for pet projects.          <p>         Use the right tool for the job.  
      ]]></description>
    </item>
  

    <item>
      <title>26</title>
      <link>http://ephjos.io/posts/2021/11/01/index</link>
      <guid>http://ephjos.io/posts/2021/11/01/index</guid>
      <pubDate>Mon, 01 Nov 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         Sometimes, I find myself feeling deep appreciation for         well-designed libraries. This happens every time I have to write         my own matrix manipulation code. The actual mathematics is         not too difficult, and neither is the implementation (optimization         aside), but making a good interface is a pain. I will always         appreciate numpy and how seamless it is to use the ndarray.         The entire library is a great example of an extremely         performant backend and a stupid simple interface. Making such         a powerful tool so easy to use will always impress and inspire me.          <p>         I like numpy :)  
      ]]></description>
    </item>
  

    <item>
      <title>25</title>
      <link>http://ephjos.io/posts/2021/10/31/index</link>
      <guid>http://ephjos.io/posts/2021/10/31/index</guid>
      <pubDate>Sun, 31 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         From what I have read, it seems to me that the most successful people         in any field are those who are able to consistently have         deeply valuable bursts. They are not the people who slog away working         80 hour weeks for the sake of working more. Rather, they work long         hours when it will produce the desired result. Cal Newport's         <a href="https://www.calnewport.com/books/deep-work/">           Deep Work</a> touches on this idea a lot, and I can see this idea         echoed elsewhere.          <p>         It is not about doing more work, it is about doing         better work more often. This meta skill seems to be almost more         important than the "core" skill itself. For a given field, a competent         person who can routinely "go deep" will likely have better results         than someone who is better at the core skill.         After a certain level, it is not about knowing more, but about being         able to apply what you know more deeply and more often.          <hr />          <p>         Happy Halloween!  
      ]]></description>
    </item>
  

    <item>
      <title>24</title>
      <link>http://ephjos.io/posts/2021/10/30/index</link>
      <guid>http://ephjos.io/posts/2021/10/30/index</guid>
      <pubDate>Sat, 30 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I am switching back to python and trying to finish the Ray         Tracer Challenge. Had I not been detoured by different languages         and ideas, I would have been done this project at the start of Covid.         This being unfinished bothers me, so I am going to finish it. I am         not enjoying the experience as much because of this, so I hope I         can just get it done and then maybe circle back in the future to         learn new languages and tools.  
      ]]></description>
    </item>
  

    <item>
      <title>23</title>
      <link>http://ephjos.io/posts/2021/10/29/index</link>
      <guid>http://ephjos.io/posts/2021/10/29/index</guid>
      <pubDate>Fri, 29 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         As someone from the Northeast Philadelphia area, I grew up hearing         people say things like "Keep that up and they'll have to put you         in Byberry". My whole life, Byberry was just an abandoned         place where they used to send "crazy" people. It wasn't until today,         when I stumbled onto         <a href="https://allthatsinteresting.com/byberry-mental-hospital">           an article about Byberry Mental hospital</a> that I got more         detail on what actually happened there. The details in the article         paint a revolting picture of a system that completely failed those         who needed its help the most.          <p>         I have been interested         in the history of asylums and mental health care in 20th century         America since I went to the Pennhurst Asylum haunted attraction at         <a href="https://en.wikipedia.org/wiki/Pennhurst_State_School_and_Hospital">           Pennhurst State School</a>. I know many people find attractions like         this disrespectful, but I can say that it was a catalyst to education         for me. I would have never learned about the reality of what happened         at Pennhurst, Byberry, and the whole United States were it not for         this attraction. As a whole, this effort to help the mentally ill         resulted in an unquantifiable amount of damage done instead. Lives were         lost, innocent people suffered, and those responsible got away without         any punishment.          <p>         When reading about places and atrocities like this, I can't shake the         feeling that there is something similar going on today. Something that         we are doing that is so obviously abhorrent that people will look on us         with disgust in 50 years. In retrospect, these mental hospitals were         clearly failures on every level. But, why could people not see it back         then? How and why could they choose <i>not</i> to see it?          <p>         I think of Moriz Scheyer and something he said in         <a href="https://en.wikipedia.org/wiki/Moriz_Scheyer#Asylum">           Asylum</a>, written about his escape of the Nazis as an Austrian         Jew. During the occupation of France, Scheyer describe how every         non-Jew did everything that they could to make money. Even the Nazi         guards themselves would allow anything, should they be bribed with         enough money. Scheyer gives the reader the sense that the majority of         people were primarily motivated by greed and lust, either lying to         themselves about or actually oblivious to the true cause of the Nazis.         Wealthy French aristocrats threw themselves at the Nazis, just to         be aligned with those in power         <b>as they took their country by force and killed their fellow         men, women, and children</b>.          <p>         What in people allows them to ignore such horrors for personal gain?         What are we ignoring now?          <p>         What am <b>I</b> ignoring now?  
      ]]></description>
    </item>
  

    <item>
      <title>22</title>
      <link>http://ephjos.io/posts/2021/10/28/index</link>
      <guid>http://ephjos.io/posts/2021/10/28/index</guid>
      <pubDate>Thu, 28 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I hate naming things. A name never feels  <i>right</i> to me. Whether         it is a variable name, a game, an API route, or a team; names rarely         come easy to me. Names can be too long, too short, too wordy, or just         plain wrong. There always feels like there is a better name out         there. One that encapsulates the idea perfectly and is just beyond my         fingertips.          <p>         The best way forward is probably to go with my gut and stick by my         decision.   
      ]]></description>
    </item>
  

    <item>
      <title>21</title>
      <link>http://ephjos.io/posts/2021/10/27/index</link>
      <guid>http://ephjos.io/posts/2021/10/27/index</guid>
      <pubDate>Wed, 27 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         It often amazes me how much of a problem the "feature creep"         phenomenon can be. I am a sucker for bells and whistles,         especially when it comes to adding them to what I am working on.         The allure         of adding on just one more thing because "I am already in this code         and it shouldn't take too long and the result will be cool" is         strong. Like most computer science students, I read the Mythical         Man Month and the whole idea of feature creep was hammered into my         skull. It makes sense when discussing a team struggling to meet         deadlines; of course you have to abandon pet feature ideas in order         to fulfill your end of the contract. However, we were never         given much guidance on avoiding feature creep when you are the         one in control of deadlines and the product.          <p>         I've often seen this problem discussed by solo game developers. Like         most things, a game rarely starts out as a fully formed idea. There         is generally no end goal that "when the game works like this, it is done".         Games and all other interesting creations start with an idea which         eventually becomes some concrete entity. On the road between idea         and finished game are a series of stops where the developer has to ask         "What next?". New features insert themselves into the game with ease,         since it is not well-defined. Before anyone catches on, the game         becomes an abomination of 36 different mechanics that do not         work well together, resulting in a lackluster final product.          <p>         Much like physical minimalism, only introduce new "things" when         they serve a direct purpose. The mug holds coffee and is used every day;         the database stores all of our information and is accessed every         time a user logs in. Introducing extraneous and unneeded features will         have an exponential negative affect on your project, and can ultimately         overshadow the core value.          <p>         Creating something is just as much about saying yes to ideas as it         is to saying no; in fact, creating is about saying no <b>more</b>         than saying yes.  
      ]]></description>
    </item>
  

    <item>
      <title>20</title>
      <link>http://ephjos.io/posts/2021/10/26/index</link>
      <guid>http://ephjos.io/posts/2021/10/26/index</guid>
      <pubDate>Tue, 26 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I came across a post entitled         <a href="https://leancrew.com/all-this/2021/10/how-many-thursdays/">           How many Thursdays?</a> on Hackernews the other day. While the whole post is a great read,         there was one bit that stuck out to me. To summarize, the post tackles         how to answer the question of whether or not a month has 4 or 5         Thursdays in it (using an iOS Shortcut). The post then starts into         the solution with this paragraph:          <blockquote>           Since every month has 28–31 days, we know that each month has           four weeks plus 0–3 "extra" days. If the date of the first Thursday           is less than or equal to the number of extra days, there will           be five Thursdays in that month.         </blockquote>          <p>         Re-framing problems like this is what I love about computer science,         and this is a great example of it. Instead of taking the problem         straight on and just counting the number of days, there is a simple         way to rephrase the problem that makes the answer trivial. Instead of         a loop that may iterate 31 times (gasp!) the answer is as simple as         two lookups and a comparison.          <p>         This is a textbook elegant solution to me, and I think it has more to         do with reframing the question than coming up with a clever answer.         It reminds me of the first time I was able to wrap my head around         the         <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">           Monty Hall problem</a>.          <p>         Instead of focusing on the probability that         you pick the prize, focus on the probability that you initially pick a         goat. If 2/3 of the time you pick a goat, and you are always shown one         goat, switching results in you picking the car 2/3 of the time.          <p>         I think it will always fascinate me how these slight shifts in         perspective and framing can have such profound effects.  
      ]]></description>
    </item>
  

    <item>
      <title>19</title>
      <link>http://ephjos.io/posts/2021/10/25/index</link>
      <guid>http://ephjos.io/posts/2021/10/25/index</guid>
      <pubDate>Mon, 25 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         Given all of the         <a href="https://us-cert.cisa.gov/ncas/current-activity/2021/10/22/malware-discovered-popular-npm-package-ua-parser-js">           recent fun with ua-parser-js         </a>, package managers have been on my mind. In a design class         in college, I was introduced to         <a href="https://en.wikipedia.org/wiki/TRIZ">           TRIZ         </a>. TRIZ is known in English as "The Theory of Inventive Problem         Solving" and was born in the Soviet Union. The most valuable aspect         of this methodology is the idea of looking at the absolute worst         possible design for the final system. A phone that explodes, a plane         that can't fly, or a vacuum that can't suck up dirt are the kinds of         inventions drummed up during these exercises. Let's create the worst         package manager possible!          <hr>          <p>         To start, it would be great if the package manager was serving a         language with no standard library and that must run on every type         computer that exists. This ensures that there are edge-cases and bugs         aplenty which means that we will "need" a whole bunch of packages         to accomplish simple things. (Bonus points if the language is         interpreted and dynamically typed).          <p>         It would be awesome if the package manager couldn't detect         vulnerabilities in packaged code. And, it'd be even better if         it reported         <a href="https://overreacted.io/npm-audit-broken-by-design/">           completely safe code as dangerous         </a>.          <p>         Complete mayhem would be caused if the package manager sometimes         decided to install different code than what was requested. Or, we         could allow anybody to edit any package and republish whenever they         like. These two combined would guarantee that a user has absolutely no         idea what code they are bringing in.         If our community decided to treat more dependencies as though they         don't have a cost, we could start to see absurdly large         dependency trees that further exacerbate the unknown code issue.          <p>         Allow packages to define arbitrary code to be executed at install-time         so that a publisher can run whatever code they want on user's machines.          <p>         We could use a convoluted system for versioning: out with Semantic         and in with Pseudorandom Versioning! Every time the package         is published, pick a random 32 bit unsigned int and call that the new         version! Provide no notion of version order, making upgrades impossible.         Instead, always get the latest version (the only one that is available).          <p>         Provide no interface to allow a publisher to take down specific package         versions and broadcast potential vulnerabilities to downstream         dependents.          <p>         A little cherry on top would be if the tooling for interfacing with the         package manager was written poorly, sometimes crashing and often eating         up computational resources for seemingly no reason. We could even slap         a crypto miner in there to make some money!          <p>         Why stop there? Why not starting mining all of a user's local data on         their machine? Surely somebody would pay good money for it!          <hr>          <p>         While this is all intentionally hyperbolic, it is somewhat grounded and         targeted at <code>npm</code>. I am not aiming to         paint <code>npm</code> as the worst         possible version of a package manager that could ever be written; rather         I want to share the areas where there is an alarming amount of overlap         in my view. <code>npm</code> has provided great value to a whole         ecosystem of developers and by extension, their users/stakeholders.         I think there is a lot to be learned from the current state of         <code>npm</code> and what steps can be taken to make it a better         package manager. Even if it does not last, we can take these lessons         and use them to build a better package manager.         I wouldn't be bold enough to assume that I can come up with the list         of things a new package manager must get right in order to be great         (especially without much thought), but I can easily spot the things         not to do.          <p>         My most immediate concern is the ability for packages to execute         arbitrary code on install. I don't see how this needs to be a feature,         and I can't drum up any convincing argument in favor of it. A package         manager should allow a developer to grab a bundle of code that they         can then use as they wish.          <p>         There is a happy middle ground between         package size and the average number of dependencies. As of now, most         packages are tiny which allows them to be easily reviewed and audited.         However, this pushes the complexity onto the dependency tree itself.         It is near impossible to review thousands of nested dependencies by         hand. At the same time, one monolithic library is subject to the whims         of whoever controls it. There is no complex tree to look through,         but now there are tens (if not hundreds) of thousands of lines of         code to review. Somewhere in the middle there is room for moderately         sized packages with a handful of dependencies that are also similarly         sized. Both the packages and dependency tree can be reviewed in         a reasonable amount of time and trusted.          <p>         I think everyone who uses <code>npm</code> would agree that this         could all be much better, but I don't see how we get there.  
      ]]></description>
    </item>
  

    <item>
      <title>18</title>
      <link>http://ephjos.io/posts/2021/10/24/index</link>
      <guid>http://ephjos.io/posts/2021/10/24/index</guid>
      <pubDate>Sun, 24 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         For as long as I have had some form of responsibility, I         have dreaded Sundays. I would always find myself becoming         stressed or upset at some point in the afternoon, feeling the         freedom of the weekend falling away. Eventually, Monday would         come and drag along with it classes, obligations, and eventually         work. My understanding is that this is a common way to feel about         the end of the weekend, and that most people attribute it to not         enjoying what Monday brings. I don't think this is the case, at         least for me.          <p>         I think this sense of dread comes from feeling like the weekend was         missed or not properly used. The times where I've felt this most         strongly were when I was not intentional with my time on the weekend.         Inversely, when I am intentional about my time on the weekend I find         myself looking forward to Monday more. This weekend was fairly action         packed and ended with a solid chunk of hours mindlessly relaxing; I am         excited to get back to work and solving problems tomorrow. I've also         felt this way after weekends spent doing absolutely nothing.         The problem comes from the weekend feeling like wasted potential,         not having been properly used. I feel best when I feel like I have         gotten something out of my time on the weekend.          <p>         Your enjoyment of Monday is a function of how you spent the weekend,         more than what Monday will bring. Being intentional with time in         the present prevents the future from "taking" that time away.  
      ]]></description>
    </item>
  

    <item>
      <title>17</title>
      <link>http://ephjos.io/posts/2021/10/23/index</link>
      <guid>http://ephjos.io/posts/2021/10/23/index</guid>
      <pubDate>Sat, 23 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I found the fantastic personal website of Derek Sivers yesterday.         Just from the         brief look I've taken around the website, I can tell that there is a         bunch more reading to be done.         While browsing I stumbled on to his post about         <a href="https://sive.rs/daydream">daydreaming</a>.         While short, I found it interesting as I've largely stopped daydreaming.         There is always some form of stimulus between computers, phones, and         TVs that there is no room for boredom. Even when I can get hooked on         a book, which feels like a productive use of time, I am still         <i>consuming</i> information. The final line of this post has been         bouncing around in my head since I read it:          <blockquote>           <b>We’ve all had plenty of input.</b>           It’s fun to let your mind direct its own entertainment.         </blockquote>          <p>         Less input, more output.          <hr />          <p>         And, since I don't think I've actually committed to it in writing, my         plan for these posts is this.          <ul>           <li>             I will post everyday for 30 days straight.           </li>           <li>             After that, there will be no fixed schedule but this is where             all of my shorter posts will live. The main blog is where all             longer form writing will go.           </li>         </ul>  
      ]]></description>
    </item>
  

    <item>
      <title>16</title>
      <link>http://ephjos.io/posts/2021/10/22/index</link>
      <guid>http://ephjos.io/posts/2021/10/22/index</guid>
      <pubDate>Fri, 22 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         This morning I came across an interesting post from Dan Luu on         <a href="https://danluu.com/look-stupid/">looking stupid</a>. Some of         the points brought up in this post resonate with me and I can         understand the arguments being made for "looking stupid". From my         perspective, this manifests most often due to asking extremely simple         (some may say stupid) questions. I've noticed that the brightest people         I have been around tend to ask alarmingly basic questions. Go to any         martial arts gym and you will find that the most experienced and         knowledgeable people in the room are the ones asking the most         questions. There are a few reasons why I think this is the case:          <ul>           <li>             Simple questions are easier to answer which allows the person asking             them to more efficiently gather information from others. Over time,             this has a powerful effect on building knowledge. The more             answers, the better.           </li>           <li>             Asking basic questions is the best way to understand why somebody             else does something. Directly asking "Why?" often doesn't             lead anywhere as even the best at something are not always             conscious of the why. It is easier to teach and share what             to do than why, as the "what" often leads to the "why".           </li>           <li>             Those that excel tend to be great students; they are able to             push down the part of them that wants to respond with "I know"             and ask the question anyway. Strangely enough, it seems             that it is possible to receive the same answer to the same question             on multiple occasions and find that the outcome is different in             each instance. A different context or the presence of new external             information can help an old answer provide new insight.           </li>           <li>             In many cases, things are much more simple than they can seem.             It can seem stupid to ask obvious questions, but there is often             an obvious answer. This prevents over-complicating simple things.           </li>         </ul>          <p>         As pointed out in the post above, if you are truly in a healthy learning         environment then looking stupid is nothing to worry about. In order to         learn, you have to put yourself out there and broadcast that you         may not know something (gasp!). Ask simple questions, especially if they seem         like they are "below" your level of knowledge. When everyone is bought         in, this leads to a free flow of information and builds trust         between those involved. This benefits the individual and the whole         alike, and is something that almost any organization can benefit from.         I think the post misses out on this aspect of "looking stupid"; it can         actually have great social value as well.          <p>         Ask simple questions and look stupid.  
      ]]></description>
    </item>
  

    <item>
      <title>15</title>
      <link>http://ephjos.io/posts/2021/10/21/index</link>
      <guid>http://ephjos.io/posts/2021/10/21/index</guid>
      <pubDate>Thu, 21 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         From my limited experience, over-communicating seems like the best         approach to default to. I find myself hesitating when feeling like I         need to ask a question. In order to ask the question comfortably, I         have to feel like I've done my work first. I hate the feeling of asking         first, only to be told the answer is trivial. However, I've also found         myself deep in a rabbit hole that I did not have to go down many times         before as well. If I were to instead err on the side of         over-communicating, I can quite easily correct back. Often, especially         in high-paced work environments, people can be missing the full picture.         By over sharing you can help prevent team members from being lost and         aid them in accomplishing their goals. Under sharing leads to         miscommunication which can have drastic consequences. Over sharing         can be a temporary inconvenience.          <p>         I was given some advice the other day:          <blockquote>           If you do not feel like you are over-communicating, then you are           not communicating enough.         </blockquote>          <p>         Over-communicate.  
      ]]></description>
    </item>
  

    <item>
      <title>14</title>
      <link>http://ephjos.io/posts/2021/10/20/index</link>
      <guid>http://ephjos.io/posts/2021/10/20/index</guid>
      <pubDate>Wed, 20 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         When working on a piece of code, I am trying to be more mindful of         assumptions. It is easy enough to write down my assumptions while         reading code. This helps me check myself, and makes it easy to ask a         teammate targeted questions that can be answered with relative ease.         What's more difficult is checking external assumptions. In conversation,         it can be tricky to navigate but useful to ask someone "Are you assuming         X?". When done correctly, this can help everybody involved get on the         same page and focus on the task at hand. We also have to watch out for         assumptions in code we are monitoring. Some of these can be obvious,         laid out in a comment. Much like listing your own assumptions, these         are easy enough to verify. The danger comes from implicit assumptions.          <p>         We've all read and written functions that make assumptions about the         parameters that they are passed. This is especially dangerous in a         dynamically typed language, where any parameter can be any value.         Static typing does not wholly prevent this problem as there may be         edge cases within a type that must be accounted for. A function can         verify that its parameter is a number, but if it divides by that number         without checking it to be non-zero, then there are still dangerous         assumptions in the code.          <p>         These dangers extend past parameters and into all logic within a         program. We make assumptions about data flow, execution order, data         models, etc. that are all core to what the code does. While some of         these are unavoidable (i.e. synchronous code executes in order) we must         be mindful of all internal and external assumptions. Err on the side         of over communicating about these assumptions to better help your         team understand the piece of code being discussed. Pay close attention         to the code you read and write and be aware of any assumptions made         within.          <p>         Always check assumptions.  
      ]]></description>
    </item>
  

    <item>
      <title>13</title>
      <link>http://ephjos.io/posts/2021/10/19/index</link>
      <guid>http://ephjos.io/posts/2021/10/19/index</guid>
      <pubDate>Tue, 19 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <img           alt="Compounding growth chart"           src="/img/compounding.webp"           width="600"           height="400">         <blockquote>           Compounding growth chart showing $100 monthly contribution           with 5% annual interest over 40 years, starting with 0 initial           investment.         </blockquote>          <p>         One of the first lessons you should learn once you become interested in         personal finance is that compounding interest is one of the most         powerful tools for building wealth. Over large time scales, small         compounding interest can have incredible effects. Getting 5% interest         on repeated savings can lead to doubling your money in about 30 years;         and lead to tripling it in 40. The gradual nature of this growth         means that gains are not apparent for the majority of the time. After         investing a significant amount of time and resources, compounding         interest starts to take over. In the above example, the 40th year of         interest provides the same gain as the first ~18 combined. Exponential         growth seems to be unintuitive and can seem insignificant, barely         greater than linear for most of the time.          <p>         I once heard the following puzzle that someone was using to help educate         people about compounding and exponential growth.          <blockquote>           There is a lake with nothing else but a single lily pad. Every day,           the amount of lily pads doubles. If it takes the lily pads 50           days to cover the entire lake, how long does it take them to cover           half of it?         </blockquote>          <p>         49 days, doubling on the last day to cover the whole lake. The         fascinating thing to me was how sudden this burst at the end is. At day         45, the lily pads only take up 3% of the lake's surface area. The growth         for these first 45 was slow and largely undetectable day-to-day. The         last 5 days see a jump from 3% to 100% coverage. After a quick search,         here is a resource with this same scenario from a University of Washington professor:         <a href="https://atmos.uw.edu/~dennis/211_Exponential_Growth.pdf">Information sheet on exponential growth</a>.          <p>         I believe that this pattern can be seen everywhere, especially in         software projects.         Small investments that can be made on a regular basis provide a         seemingly disproportionate return over time. I think most developers         have seen this on a shorter time scale, say a couple of months. First,         there are a couple months of PRs that fix bugs and refactor code         without providing much value to the end user. Then, seemingly out of         nowhere, there are a handful of PRs that have a cascading effect leading         to dozens of issues being closed. The reality is that <i>all</i> of the         work done in those few months is responsible for the fixes at the end,         just that there needed to be a significant investment before         the true return could be realized.          <p>         This is the core idea behind James Clear's         <a href="https://jamesclear.com/continuous-improvement">1% better every day</a>         idea, which was deeply profound to me the first time I came across it.          <p>         Small continuous improvements lead to compounding growth.  
      ]]></description>
    </item>
  

    <item>
      <title>12</title>
      <link>http://ephjos.io/posts/2021/10/18/index</link>
      <guid>http://ephjos.io/posts/2021/10/18/index</guid>
      <pubDate>Mon, 18 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         Today I was able to finish the canvas chapter of the Ray Tracer Challenge         in racket. I spend a good bit of time digging through the racket docs         on basic string building and was able to get everything working.         The usage of vectors for storing the pixel data feels fast, but         converting the         pixel data to the output (PPM) string is slow.         This is somewhat expected, but is still a point of concern.         There are two aspects of this process that slow it down: converting         the vector to a list and string concatenation. Ideally, I would         directly pull the data out of the vector and add it to buffer, which         could then be written to the output file. The current implementation         uses higher-level string functions that will likely crush performance         going forward. This should not bring things to a halt, and the output         should still always be generated. Regardless, I expect to be back in         this code in the near future.          <p>         If I was to do this in Java, I would pull the information directly out         of an <code>Array</code> and append it to a <code>StringBuilder</code>.         Then, the <code>StringBuilder</code> could write its contents to the         output file. This solution would be quick and easy to read. I should         be able to get similar performance out of racket, once I learn more.  
      ]]></description>
    </item>
  

    <item>
      <title>11</title>
      <link>http://ephjos.io/posts/2021/10/17/index</link>
      <guid>http://ephjos.io/posts/2021/10/17/index</guid>
      <pubDate>Sun, 17 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         Another slight mindset shift I am trying to make is to focus on         problems, not solutions. This idea is parroted around a lot especially         in online-entrepreneur-influencer circles, but I think it does have         some value. As someone who can obsess over what the "best"         solution to a problem is and make the entire process much more difficult         for         himself, this tweak may prove helpful. Instead of being occupied         purely with the solution, pour more time and effort into learning about         the problem. The classic questions of who, what, where, when, why, and         how are all of the tools needed here.          <p>         The core hypothesis behind this idea is that a good solution should be         evident from the problem, once you have enough data about the problem.         Instead of languishing about large-scale architecture mistakes and         refactors, focus on the specific issue at hand. If the original author         is still on the team, reach out to them for some information. A quick         slack message may illuminate something before unseen to you. Identify         exactly what the issue is, and try to keep this definition targeted.         Pinpoint exactly where in both the code and the product this issue         occurs. Enumerate every place where this is the case, which may be         helpful for testing at the end. Understand what decisions/mistakes led         to this issue being checked-in to the code base. List out all of the         different manners in which the issue can be caused. Is it reproducible?         Is it deterministic? Given all of this information on top of the code         itself, clearly explain how the issue occurs. From code decisions to         what manifests to the end user, you should be able to explain each step         along the way and what key pieces lead to the final result.          <p>         With this detailed of a view on the problem, a list of potential         solutions should become immediately clear. Rank these solutions and         get feedback from team members (if necessary) before moving forward with         the best choice. Don't treat this is as a binding agreement: more         information may become available while working on this fix that reveals         another solution as a better choice. If unconstrained by time, feel free         to switch to the new approach.          <p>         Instead of focusing on the solution, obsess over the problem. Gather         info and make an educated decision on how to best tackle the problem.         The end goal is to fix the issue, not to create an elegant solution.         If the investigation into the problem is done right, this should         happen largely on its own.          <p>         Well defined problems reveal elegant solutions.  
      ]]></description>
    </item>
  

    <item>
      <title>10</title>
      <link>http://ephjos.io/posts/2021/10/16/index</link>
      <guid>http://ephjos.io/posts/2021/10/16/index</guid>
      <pubDate>Sat, 16 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         When working on a large project, knowing whether or not a pull request         is "good" can be difficult. We can find ourselves in abstract         discussions of what the best approach should be or leaving review         that "maybe you could fix X too". While these conversations have value,         they are not pragmatic and don't directly lead to code changes and PRs         being merged.          <p>         I am working on having a mindset focused on incremental improvement.         If a PR makes an undeniable improvement to the code base and does not         introduce any bugs or debt, then it should         be merged. If every PR can provide some improvement and be turned around         quickly, a team can rapidly rid themselves of the problems they spend         so much time discussing.          <p>         It is hard to walk the line between "fix everything" and "make         the project better", but I think it can (and should) be done. Identify         the core issues, create PRs to fix them, and merge the PRs. The ideal         PR is likely         <i>small and targeted</i> instead of         <i>large and perfect</i>. Over time, the smaller PRs can be merged faster,         provide a smaller surface area for bugs, and approach the perfect solution.  
      ]]></description>
    </item>
  

    <item>
      <title>9</title>
      <link>http://ephjos.io/posts/2021/10/15/index</link>
      <guid>http://ephjos.io/posts/2021/10/15/index</guid>
      <pubDate>Fri, 15 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         After briefly getting into racket's vectors, I am becoming a bit         more hopeful of finishing the Ray Tracer Challenge in racket. They         are default mutable         and come with analogs for most of the standard list functions (map,         filter, etc.). While I like pure functional programming and understand         the value, mutability can provide great performance improvements         in some cases (like         ray tracing). I'll be using vectors in the canvas struct, which contains         an RGB tuple for each pixel in the image we are trying to generate.         I believe that the source of the issues I ran into in Haskell was precisely         writing pixels to the canvas. Instead of having to copy the canvas to         do writes, simply get a reference to the pixel data and update a color         at a specific location. No need to worry about passing around and using         as State monad (which I still have not figured out how to do), just update         the value in place. This is exciting!          <p>         On a related note, I can never figure out a naming for "mathematical         vectors" when using them alongside vector containers. I need to use         the standard         library's <code>make-vector</code> to create the pixel data container.         This meant that I needed to rename my 3D vector (and point since they         are both "tuples"). I've gone with <code>make-vector-3d</code> in         this case, but I'd rather call the container an <i>ArrayList</i>         instead.         Or, since "list" is the default container type         in racket, <i>Array</i> would work just fine and make sense. 
      ]]></description>
    </item>
  

    <item>
      <title>8</title>
      <link>http://ephjos.io/posts/2021/10/14/index</link>
      <guid>http://ephjos.io/posts/2021/10/14/index</guid>
      <pubDate>Thu, 14 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I love simple tools.          <p>         Reflecting on some of what I talked about         <a href="/posts/2021/10/13">yesterday</a>,         I realized something else about myself. While friction and         resistance can be crippling, having too much freedom can be just as         bad. I notice this particularly with tooling I use often. Bells and         whistles do a great job of distracting me, and I will sink a         disproportionate amount of time into something that is practically         insignificant.          <p>         I think this part of my personality is what drew me to Linux.         The opportunity for endless customization and control was exciting         and pulled me in. I have spent countless hours customizing different         dotfiles and settings, a lot of which I use less than once a month.         While this is great educational value in this level of curiosity,         it does sometimes get in the way.          <p>         I've found that by focusing on a small set of simple and extendable         tools, I can get myself to focus on the problems I am trying to solve         much easier. Now that I am familiar with these tools, I can use, tweak,         update, and fix them without worry and (largely) without distraction.         There are fewer moving parts, no batteries to charge, no oil to change:         just a hammer to swing at a problem. While I understand and still         feel the allure of cool screenshots and retro programs, I find I enjoy         using a computer more by keeping it simple.  
      ]]></description>
    </item>
  

    <item>
      <title>7</title>
      <link>http://ephjos.io/posts/2021/10/13/index</link>
      <guid>http://ephjos.io/posts/2021/10/13/index</guid>
      <pubDate>Wed, 13 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         Last year, in the first summer of the pandemic, I listened to James         Clear's         <a href="https://jamesclear.com/atomic-habits">Atomic Habits</a> on         audiobook during my workouts. I found the ideas discussed within         to be fascinating and their application to be more intriguing. Of all         of the ideas discussed in the book, the most well known is probably the         idea of habit chaining. The idea here is that it is near impossible to         take on a bunch of new habits simultaneously, especially when they are         all disconnected. Instead, it is easier to "attach" a habit to an         existing one. If you want to write every morning, and you already have         the habit of waking up and making a cup of coffee, you can try chaining         writing onto the existing habit. Instead of trying to "write a blog post         everyday", aim to "write for 10 minutes <i>after</i> making coffee". This is         a smaller habit that builds up your existing system. There is lower         friction when trying to start the new task, since you are riding on         the momentum of your existing habit. With coffee in hand, you are         already prepped for and 10 minutes of writing away from completing         the task. Once you are writing everyday, it is easy to increase the         amount of time spent writing since you already have the habit of starting.          <p>         Remove the need         for motivation and instead make what you want to do easy, and what         you don't want to do hard. Its easier to eat healthy if you only have         healthy foods in the house; it is hard to east unhealthy if you only         have healthy foods in the house. This resonates with me the most         since I tend to <i>over-perceive</i> the amount of friction involved         in tasks.          <p>         When not part of a habit, simple tasks can become daunting and take         me a while to decide to do. I find that by trying to see the         true level of friction, I can "motivate" myself to do these tasks much         easier. Instead of being worried about how complicated a new feature         may be and how long it will take to implement and all of the potential         bugs that may be introduced: just start. Version control is a great         tool which allows us to undo or fix changes with ease.          <p>         For me, habits are best built by lowering the cognitive effort necessary         to start them, increasing the friction of the opposite task, and spending         what cognitive effort is necessary to analyze the true friction of the         task. It is less about finding the motivation than it is about         overcoming this "resistance" towards all tasks. Given these traits,         I seem to function best by simplifying the tools and systems around         me. I've found that         <a href="https://en.wikipedia.org/wiki/KISS_principle">"Keep It Simple, Stupid"</a>         tends to work for me. 
      ]]></description>
    </item>
  

    <item>
      <title>6</title>
      <link>http://ephjos.io/posts/2021/10/12/index</link>
      <guid>http://ephjos.io/posts/2021/10/12/index</guid>
      <pubDate>Tue, 12 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         After some discussions about unit tests at work yesterday, I came across         this solid <a href="https://softwareengineering.stackexchange.com/a/356238">StackExchange</a>         post. It turns out my understanding of the ways that a unit test can be         brittle were fairly limited, meaning I missed a few of the bad ways to         write unit tests. In particular, I previously would add in additional         checks that "just made sure" for different values/functionality along         the way. Often, these checks were almost completely orthogonal to the target         of the unit test. I saw this as more stable (more tests = better)         not as more brittle. But, now I understand the perspective that         extraneous assertions can lead to tests failing even when their "core"         tests pass. A rule of thumb to try going forward: strive to have         only 1 assert per unit test. While this is admittedly impractical and         not always the best approach, I think it is the kind of correction         I need to make in my approach.          <p>         On a different note, Ethan Chlebowski's recipe for         <a href="https://www.ethanchlebowski.com/cooking-techniques-recipes/street-cart-chicken-amp-yellow-rice">halal cart style chicken and rice</a>         is fantastic. I didn't think it would be so easy to recreate a         freshman year favorite of mine at home. If I exercise control         by making the salad portion bigger, rice portion smaller, and using a touch less         sauce, this is actually a solid healthy meal.          <p>         The most important         contribution Ethan has made to my cooking is his mayo marinade technique         which is applied in the above recipe. Instead of oil, marinate your         meat in a seasoned mayo which is oil+egg+vinegar. This results in         more tender, juicer meat with the bonus of not having to worry about         putting fat in the pan: the meat brings enough fat to cook it from         the marinade. The usage of mayo here also provides incredible color         to the finished product, which is always great to have. 
      ]]></description>
    </item>
  

    <item>
      <title>5</title>
      <link>http://ephjos.io/posts/2021/10/11/index</link>
      <guid>http://ephjos.io/posts/2021/10/11/index</guid>
      <pubDate>Mon, 11 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         Thanks to Tyson Fury vs. Deontay Wilder 3 causing me to stay up until         1:30am Saturday night, I am beat this morning. It was well worth it.         I am hoping a good workout         and some caffeine will help before the real work of the day begins.          <p>         I have found myself spending more and more time reviewing PRs, which         has been a great experience. This doesn't feel like it is cutting into         my other work and is helping me keep up with my team and get a better         understanding of the project as a whole. By time-boxing it to an hour         or two a day, I can ensure that I have time to fulfill my personal         responsibilities while still being accountable to the team. To         me, the biggest benefit of PR review besides QA and bug mitigation         is the sharing of knowledge.          <p>         A good PR should cleanly explain what the changes are, why they were         made, and what issues still exist in the affected code or linger         around the edges. This gives the reviewer a clear picture of the         motivation, allowing them to more easily see if the code completes         its objectives. The why gives insight into that developer's         perspective on the project as a whole, the issue in specific, and         their general problem solving approach. This can be incredibly         insightful, especially as a relatively new engineer, as it provides         a clear window into how my colleagues see things. When done well,         this can be a direct transfer of skills and knowledge that allows         the reviewer to become a better developer themselves.          <p>         Finishing with         the remaining issues and other concerns helps share that developer's         perspective on what work still has to be done. This spreads         project specific information which can help eliminate borders in         knowledge. It is never good to have a part of the code that         "only X understands, go ask them". While levels of expertise and         understanding vary, everyone on the team should have a minimally         workable understanding of each part of the codebase. This makes         the team more resilient to outside events while again         pushing each member's boundaries and ultimately making them better         at what they do.          <p>         If a team can consistently open concise, well explained PRs         that provide meaningful information and definitive improvements to         their project, both the final product and the engineers themselves will         improve. Do this for a period of time and a deeply valuable product         and team can be built. To be kitschy:         <blockquote>           software development is about developing software as much as           it is about developing the people who create it         </blockquote>  
      ]]></description>
    </item>
  

    <item>
      <title>4</title>
      <link>http://ephjos.io/posts/2021/10/10/index</link>
      <guid>http://ephjos.io/posts/2021/10/10/index</guid>
      <pubDate>Sun, 10 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I've hit that point in implementing a ray tracer where I need to         start implementing structs that have 2D arrays. This is needed         for the canvas' pixels and eventually in the matrix implementation.         This is straightforward enough in an imperative language, but has been         a sticking point in the past.          <p>         After some brief research, Racket's mutable vectors seem easy enough to         use, and are probably exactly what I need in this case. I want to         try to avoid building up thunks in these areas of the code as that         can lead to a massive space leak (as I've learned in the past).          <p>         If Racket's vectors are anything like those from Haskell, I'll         most likely wind up falling back to Python just to finish this project.          <p>         Wish me luck. 
      ]]></description>
    </item>
  

    <item>
      <title>3</title>
      <link>http://ephjos.io/posts/2021/10/09/index</link>
      <guid>http://ephjos.io/posts/2021/10/09/index</guid>
      <pubDate>Sat, 09 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         I've always wanted to write a somewhat substantial program         using a Lisp. I was first introduced to the family of languages in         a programming languages course: good-old MIT Scheme. This was         such a foreign experience at the time both around tooling and the         language itself. This opened up a whole other world of programming         languages that led to me finding Haskell and Racket. However,         I've never written anything too complex with either (aside from the         <a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/sicp/book/node76.html">SICP metacircular evaluator</a>).          <p>         There has been an ongoing cycle of start-stop-delete with my attempts at         <a href="https://pragprog.com/titles/jbtracer/the-ray-tracer-challenge/">The Ray Tracer Challenge</a>,         which will warrant a more lengthy post. These days, I am working through         this book in Racket. Just in the first chapter I've learned a bit more         about the language and its tooling, and I look forward to the rest.          <p>         I am thinking about stopping and just crunching through the book in         Python to get myself all the way through it. Maybe a first pass with         relatively low friction will help me actually get through it using         the other languages I have in mind. So far focusing on reducing friction         and finsihing things has served me well, so maybe that's what it needed         here.          <p>         I also still need to bring over a couple of my old blog posts to this         version of the site. 
      ]]></description>
    </item>
  

    <item>
      <title>2</title>
      <link>http://ephjos.io/posts/2021/10/08/index</link>
      <guid>http://ephjos.io/posts/2021/10/08/index</guid>
      <pubDate>Fri, 08 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
              <p>         So about that whole "rss feed by hand" thing.          <p>         I spent some time this morning writing a couple bash scripts         that iterate over the files for both blogs and         generates each their own rss feed <i>and</i> index page. This greatly         reduces the amount of work necessary for each post, while also forcing         me to stay in this simple format.          <p>         Yes, I am looking at the inflexibility of a hacked together bash script         as a feature, not a bug in this case. I also committed the sin of         using regex to parse text out of HTML, <a href="https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454"         >but I think I'll be okay</a>.          <p>         All in all, I am pretty happy with how everything works right now. To         create a post I copy my template file to the new post, write it, and         run a bash script before pushing git and docker. As long as I follow         some simple formatting that my scripts expect, all should be well.          <p>         I also noticed some super strange caching behavior when testing the         site on my other devices yesterday, so I am pushing a change to stop         caching. These pages are small and lightweight enough that it shouldn't         matter, future me can worry about that. 
      ]]></description>
    </item>
  

    <item>
      <title>1</title>
      <link>http://ephjos.io/posts/2021/10/07/index</link>
      <guid>http://ephjos.io/posts/2021/10/07/index</guid>
      <pubDate>Thu, 07 Oct 2021 12:00:00 -0400</pubDate>
      <description><![CDATA[
            <p>       This is my first micro post. I am not really sure what this       will turn out to be, but I do want to see how far I can take it.       This will just be a place to jot down what's on my mind at       the time. Posts like projects or other more involved writing       will go in the main blog.        <p>       As of now, I am working on laying the website out again (for I       believe the 6th time). I am keeping the majority of the       UI/UX the same, but moving completely to a plain HTML/CSS/JS       stack.        <p>       I found out about SSI (Server Side Includes) the other day,       so I don't even need to use an external templating system.       Just set <code>ssi on</code> in nginx and get up and running.       This lets me share the head, navbar, and footer between pages       easily which was always my main deterrent from dropping all       the way down to bare HTML.        <p>       I've previously told myself that I don't like writing HTML and       that I <b>needed</b> a markdown setup in order to write, but       that certainly didn't lead to more writing.        <p>       So now, everything is dead simple. These posts don't       even really need markup, and in the cases that they do I'm       sure I'll get around to adding some useful snippets to my       vim config.        <p>       Creating the index pages and rss feed by hand is a bit strange,       but the whole theme of this effort is to <i>just do it</i>.        <p>       Here it goes! 
      ]]></description>
    </item>
  

</channel>
</rss>

